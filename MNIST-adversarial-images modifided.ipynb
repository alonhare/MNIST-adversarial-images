{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies for entire notebook here\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for creating weights and biases\n",
    "# https://www.tensorflow.org/get_started/mnist/pros\n",
    "def weight_variable(shape, name=None):    \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "# Functions for convolution and pooling functions\n",
    "def conv2d(x, W, name=None):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME', name=name)\n",
    "\n",
    "def max_pooling_2x2(x, name=None):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "def load_saved_adversarial():    \n",
    "    local_path = \"C:\\\\Users\\\\alonh\\Documents\\\\Thesis\\\\MNIST-adversarial-images\\\\\"\n",
    "    original_number = np.load(local_path + \"original_number.npy\")\n",
    "    target_number = np.load(local_path + \"target_number.npy\")\n",
    "    prob_list = np.load(local_path + \"prob_list.npy\")\n",
    "    adv_img_list = np.load(local_path + \"adv_img_list.npy\")\n",
    "    return original_number, target_number, prob_list, adv_img_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_adversarial\n",
    "original_number, target_number, prob_list, adv_img_list = load_saved_adversarial()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniting and shuffeling between adv inputs and benign\n",
    "# creating labels as 2d array which [0,1] is adv\n",
    "number_of_adv_images = adv_img_list.shape[0]\n",
    "adv_label = np.zeros(number_of_adv_images)\n",
    "adv_label = np.c_[adv_label, np.ones(number_of_adv_images)]\n",
    "\n",
    "number_of_benign = 1000\n",
    "benign_label = np.ones(number_of_benign)\n",
    "benign_label = np.c_[benign_label, np.zeros(number_of_benign)]\n",
    "# uniting adv and benign\n",
    "united_labels = np.concatenate((adv_label, benign_label), axis=0)\n",
    "united_images = np.concatenate((adv_img_list, mnist.train.images[0:number_of_benign]), axis=0)\n",
    "united_images_and_labels = np.c_[united_images, united_labels]\n",
    "np.random.shuffle(united_images_and_labels)\n",
    "#add precondition\n",
    "images_data, labels_data = np.hsplit(united_images_and_labels, [united_images_and_labels.shape[1]-2])\n",
    "train_test_rate = 0.7\n",
    "train_size = int(images_data.shape[0]*train_test_rate)\n",
    "image_train_data, image_test_data = np.split(images_data, [train_size])\n",
    "label_train_data, label_test_data = np.split(labels_data, [train_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable W_conv1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-4206db985b6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# should start run from here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mW_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"W_conv1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mb_conv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"b_conv1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mW_conv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"W_conv2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1465\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1215\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    525\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    479\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    846\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 848\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    849\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable W_conv1 already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# should start run from here\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "W_conv1 = tf.get_variable(\"W_conv1\", shape=[5, 5, 1, 32])\n",
    "b_conv1 = tf.get_variable(\"b_conv1\", shape=[32])\n",
    "W_conv2 = tf.get_variable(\"W_conv2\", shape=[5, 5, 32, 64])\n",
    "b_conv2 = tf.get_variable(\"b_conv2\", shape=[64])\n",
    "saver = tf.train.Saver(var_list={'W_conv1': W_conv1, 'b_conv1': b_conv1, 'W_conv2': W_conv2, 'b_conv2': b_conv2})\n",
    "saver.restore(sess, \"C:\\\\Users\\\\alonh\\Documents\\\\Thesis\\\\MNIST-adversarial-images\\\\original-nn-data.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders nodes for images and label inputs\n",
    "# Create placeholders nodes for images and label inputs\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='xDetector')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2], name='yDetector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified nn based on original\n",
    "to_initialize = []\n",
    "# Input layer\n",
    "x_image_new = tf.reshape(x, [-1,28,28,1]) # mnist image comes in as 784 vector\n",
    "\n",
    "# # Conv layer 1 - 32x5x5 - using same weights and biases as the pretrained\n",
    "# W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# b_conv1 = bias_variable([32])\n",
    "x_conv1 = tf.nn.relu(conv2d(x_image_new, W_conv1) + b_conv1, name=\"x_conv1_new\")\n",
    "x_pool1 = max_pooling_2x2(x_conv1, name=\"x_pool1_new\")\n",
    "# Conv detector layer 1  - 32x5x5\n",
    "W_conv1_detector = weight_variable([5, 5, 32, 32], name=\"W_conv1_detector\")\n",
    "b_conv1_detector = bias_variable([32], name=\"b_conv1_detector\")\n",
    "x_conv1_detector = tf.nn.relu(conv2d(x_pool1, W_conv1_detector) + b_conv1_detector, name=\"x_conv1_detector\")\n",
    "x_pool1_detector = max_pooling_2x2(x_conv1_detector, name=\"x_pool1_detector\")\n",
    "\n",
    "# # Conv layer 2 - 64x5x5\n",
    "# W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "# b_conv2 = bias_variable([64])\n",
    "x_conv2 = tf.nn.relu(conv2d(x_pool1, W_conv2) + b_conv2, name=\"x_conv2_new\")\n",
    "x_pool2 = max_pooling_2x2(x_conv2, name=\"x_pool2_new\")\n",
    "\n",
    "# stacking conv2 output and x_pool1 detector output\n",
    "conv_detector_2_input = tf.concat([x_conv2, x_pool1], 3)\n",
    "\n",
    "\n",
    "# Conv detector layer 2 - 64x5x5\n",
    "W_conv2_detector = weight_variable([5, 5, 96, 64], name=\"W_conv2_detector\")\n",
    "b_conv2_detector = bias_variable([64], name=\"b_conv2_detector\")\n",
    "x_conv2_detector = tf.nn.relu(conv2d(conv_detector_2_input, W_conv2_detector) + b_conv2_detector, name=\"x_conv2_detector\")\n",
    "x_pool2_detector = max_pooling_2x2(x_conv2_detector, name=\"x_pool2_detector\")\n",
    "# Flatten - keras 'flatten'\n",
    "pool_shape = x_pool2_detector.shape\n",
    "shape_size = (pool_shape[1]*pool_shape[2]*pool_shape[3]).value\n",
    "x_flat_detector = tf.reshape(x_pool2_detector, [-1, shape_size], name=\"x_flat_detector\")\n",
    "\n",
    "\n",
    "W_fc1_detector = weight_variable([shape_size, 1024], name=\"W_fc1_detector\") # max pooling reduced image to 7x7\n",
    "b_fc1_detector = bias_variable([1024], name=\"b_fc1_detector\")\n",
    "x_fc1_detector = tf.nn.relu(tf.matmul(x_flat_detector, W_fc1_detector) + b_fc1_detector, name=\"x_fc1_detector\")\n",
    "\n",
    "# Regularization with dropout\n",
    "keep_prob_detector = tf.placeholder(tf.float32, name=\"keep_prob_detector\")\n",
    "x_fc1_drop_detector = tf.nn.dropout(x_fc1_detector, keep_prob_detector, name=\"x_fc1_drop_detector\")\n",
    "\n",
    "\n",
    "# Classification layer\n",
    "W_fc2_detector = weight_variable([1024, 2], name=\"W_fc2_detector\")\n",
    "b_fc2_detector = bias_variable([2], name=\"b_fc2_detector\")\n",
    "y_conv= tf.matmul(x_fc1_drop_detector, W_fc2_detector) + b_fc2_detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities - output from model (not the same as logits)\n",
    "# CHANGE!!-----------------------------------------------------------------\n",
    "y = tf.nn.softmax(y_conv, name=\"y_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cross_entropy2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv), name=\"cross_entropy2\")\n",
    "# add variables to optimize here:\n",
    "var_list = [W_conv1_detector, W_conv2_detector, W_fc2_detector, b_conv1_detector, b_conv2_detector, b_fc2_detector]\n",
    "train_step2 = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy2, var_list=var_list, name=\"train_step2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to test accuracy of model\n",
    "correct_prediction2 = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1), name=\"correct_prediction2\")\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32), name=\"accuracy2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized_global_variables(sess):\n",
    "    \"\"\"\n",
    "    Only initializes the variables of a TensorFlow session that were not\n",
    "    already initialized.\n",
    "    :param sess: the TensorFlow session\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # List all global variables\n",
    "    global_vars = tf.global_variables()\n",
    "\n",
    "    # Find initialized status for all variables\n",
    "    is_var_init = [tf.is_variable_initialized(var) for var in global_vars]\n",
    "    is_initialized = sess.run(is_var_init)\n",
    "\n",
    "    # List all variables that were not initialized previously\n",
    "    not_initialized_vars = [var for (var, init) in\n",
    "                            zip(global_vars, is_initialized) if not init]\n",
    "    # for uninit in not_initialized_vars:\n",
    "    #     print(uninit)\n",
    "    # Initialize all uninitialized variables found, if any\n",
    "    if len(not_initialized_vars):        \n",
    "        sess.run(tf.variables_initializer(not_initialized_vars)) \n",
    "\n",
    "initialize_uninitialized_global_variables(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.56\nstep 2, training accuracy 0.5\nstep 4, training accuracy 0.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6, training accuracy 0.48\nstep 8, training accuracy 0.49\nstep 10, training accuracy 0.56\nstep 12, training accuracy 0.49\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "# Run once to get the model to a good confidence level\n",
    "batch_size = 100\n",
    "batch_index = 0\n",
    "image_train_size = image_train_data.shape[0]\n",
    "number_of_steps = image_train_size / batch_size\n",
    "for i in range(int(number_of_steps)):\n",
    "    # if(batch_index>=image_train_data.shape[0]):\n",
    "    #     print(\"reached limit of train data current index: \", batch_index, \"data size: \", image_train_data.shape[0])\n",
    "    #     break\n",
    "    batch_image = image_train_data[batch_index: (batch_index + batch_size)]\n",
    "    batch_label = label_train_data[batch_index: (batch_index + batch_size)]\n",
    "    if i % 2 == 0:\n",
    "        train_accuracy = accuracy2.eval(session=sess, feed_dict={x: batch_image, y_:batch_label, keep_prob_detector: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "    train_step2.run(feed_dict={x: batch_image, y_: batch_label, keep_prob_detector: 0.4})\n",
    "    batch_index = batch_index + batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.517544\n"
     ]
    }
   ],
   "source": [
    "# Run trained model against test data\n",
    "print(\"test accuracy %g\"%accuracy2.eval(session=sess, feed_dict={x: image_test_data, \n",
    "                                                  y_: label_test_data, keep_prob_detector: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
