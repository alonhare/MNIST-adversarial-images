{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Adversarial Images with TensorFlow\n",
    "\n",
    "Create adversarial images to fool a MNIST classifier in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tech Challenge & Benchmark\n",
    "\n",
    "**Backstory**\n",
    "\n",
    "The original concept of this notebook was based on a Machine Learning (intern) candidate tech challenge from the Toronto startup [500px](https://500px.com).\n",
    "\n",
    "When I first saw the posting, it was at the beginning of my 3 month career pivot into Deep Learning and I thought this challenge would be a great way for me to benchmark my progress once I get started. You can read more about my career transition journey on [Medium](https://medium.com/towards-data-science/my-3-month-deep-learning-career-pivot-af94cd8d6a31) and a revised/updated version on [LinkedIn]().\n",
    "\n",
    "Although, I didn't follow through with providing the entire final output of the challenge, I'm quite satisfied that I've successfully completed it and consider it a demonstration of my current knowledge and capability.\n",
    "\n",
    "Prior to starting this challenge, I completed [Fast.ai: Practical Deep Learning - Part 1](http://course.fast.ai/). Read through my blog post to see my reading material - [Deep Learning Reading List](http://jasonicarter.github.io/deep-learning-reading-list).\n",
    "\n",
    "**The Challenge (summarized)**\n",
    "\n",
    "Create adversarial images to fool a MNIST classifier in TensorFlow.\n",
    "1. Learn how adversarial examples are created. For example, “Breaking Linear Classifiers on ImageNet” gives a good overview on the subject.\n",
    "2. Install Tensorflow\n",
    "3. Follow “Deep MNIST for Experts” tutorial to get the MNIST classifier running.\n",
    "4. Expand the code from the previous step to generate adversarial images. Specifically, pick 10 images of digit ‘2’ which are correctly classified as ‘2’ by the trained model and modify them so the network incorrectly classifies them as 6.\n",
    "5. Generate adversarial examples and save them as a single image containing a grid of 10 rows and 3 columns. The rows correspond to the selected examples of ‘2’. The columns are original image, delta and adversarial image. Provide link to the resulting image.\n",
    "6. Make your code clean and readable. Add comments where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "1. Read basic need-to-know about adversarial images\n",
    "2. Get data to be used throughout notebook\n",
    "3. Build a simple CNN (test model)\n",
    "4. Train it on MNIST \n",
    "5. Show model classification of 10 handwritten 2s\n",
    "6. Create adversarial image to classify 2s as 6s\n",
    "7. Test adversarial image with original model (in step 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies for entire notebook here\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-d651cc53d7d8>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.images shape:  (55000, 784)\nTraining.labels shape:  (55000, 10)\nShape of an image:  (784,)\nExample label:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Take a look the training data\n",
    "print('Training.images shape: ', mnist.train.images.shape)\n",
    "print('Training.labels shape: ', mnist.train.labels.shape)\n",
    "print('Shape of an image: ', mnist.train.images[0].shape)\n",
    "print('Example label: ', mnist.train.labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE/CAYAAAAnhFRiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XeYVcX5wPHvy+7C0qsiLGXpig0MWKJRErBERfzFoBgLJuhGxUQSNRBTLDEJMRZUbCgqEhWJDaxECUgsQSRWpCogvfe27O78/jh3Zw7s3d3by7nv53n22ffOPfecuXd2586cOTNHjDEopVRQ1El3BpRSKpG0UlNKBYpWakqpQNFKTSkVKFqpKaUCRSs1pVSgBL5SE5GZInJlql+rDqTlkBlyoRyyplITkWUiMiDd+aiOiDwiIjt9P/tEZEe685VoWVAOQ0RkoYhsE5H1IjJBRJqkO1+Jlunl4Cci/xYRIyL5qThe1lRqmc4Yc7UxplHlD/Ac8M905ysHvQ+cbIxpCnQG8oE70pul3CUil+CVQcpkfaUmIs1F5DUR2SAiW0Jxu4M26yIiH4W+vaeISAvf608UkQ9EZKuIfCYi/RKQp4bABcCEePeVLTKlHIwxK4wxG31J5UDXWPaVjTKlHEL7agrcAvwm1n3EIusrNbz38CTQEegA7AHGHrTN5cDPgLZAGXA/gIgUAa/jfZO3AG4EXhSRQw4+iIh0CBV0hwjydAGwAZgVyxvKUhlTDiJyiohsA3bglcWY+N5aVsmYcgD+AjwMrI3nDUXNGJMVP8AyYEAE2/UCtvgezwRG+x73BEqBPGAkMPGg108Dhvpee2UMeZ0O3Jruz0zLgSLgVqB7uj+3XCsHoA/wKV7XsxgwQH4qPpusb6mJSAMReVRElovIdrzWUTMRyfNttsIXLwcKgFZ432aDQ984W0VkK3AK0CaO/LQHTgOejnUf2SjTygHAGLMKeAuYFM9+skkmlIOI1AEeAq43xpTF835ikdITeElyA9ADOMEYs1ZEegGfAOLbpr0v7gDsBzbiFe5EY8xVCczP5cAHxphvErjPbJBp5VApH+iShP1mqkwohyZ4LbXnRQS8ViDAShEZbIz5T5z7r1G2tdQKRKTQ95MPNMY7b7A1dMLzljCvu1REeopIA+B24AVjTDnwD2CgiJwpInmhffYLc2I1GpcDT8Xx+myQseUgIpeEzveIiHQE/ox3OiCIMrUctuGdr+sV+jk7lP4dYHb0bzM62VapvYFXYJU/t+KdBK6P903zX7zuxsEm4lU0a4FC4JfgjZQBg4Cb8U7srwBuIsznEvpH2VnLCeqTgHYE/1KOTC6HnsAHwE68yzsWAsloAWaCjCwH41lb+RPaF8A6Y0xprG82UhI6qaeUUoGQbS01pZSqkVZqSqlA0UpNKRUocVVqInKWeJOHl4jIqERlSimlYhXzQEHoYr5FwOnASmAOcLEx5qvEZU8ppaITz8W3xwNLKi8yFZFJeMPB1VZqdaWeKaRhHIfMLXvZRanZJ7VvGR0th+gkoxy0DKK3gy0bjTFV5qEeLJ5KrYgDp1usBE44eCMRKQFKAAppwAnSP45D5pbZJnHXjGo5xC5R5aBlEJ93zAvLI9kunnNq4b65qvRljTHjjDF9jDF9CqgXx+FUPLQc0k/LIDXiqdRWcuAcsnbA6viyo5RS8YmnUpsDdBORTiJSFxgCTE1MtpRSKjYxn1MzxpSJyHV46y3lAU8YY+YlLGdKKRWDuJYeMsa8gTepVimlMoLOKFBKBYpWakqpQNFKTSkVKFqpKaUCRSs1pVSgaKWmlAqUINxNSmW5vJ7dAVhwTXObtvhHD9u4wjf7rk5odt5DWzvZtAn3nG3jluM/TFo+VXbQlppSKlC0paZSJr+9u9PaV7ccZuPnfvAoAL3rVdi0Ct/3bQUuvfJ7uKTZEpvSduQzNn5i2vdsXLZyVfyZDoA6hYU27jDLrUPxUNH7Ns4T73OdX7rbpt1w5uU2Ll/oPu9Mpy01pVSgaKWmlAqUnOh+rvn1d20svhXfCjd5D7Yc7tLafFjunn/1o6TnLei+ufMkGy+45EEbhzv57+9yvr67qY0/2tm5yn6/03CZjS9otN3Gq6d9aePXjmxOLqvsdq6a5AZVXit6Juy2/b48HwC5u5VNq/f1pzEdN7/Y3d+4bNm3Me0jHtpSU0oFilZqSqlAyaju5/rhrpu49Zj9Nn75jLFx7feIunPCpu81ZQA0rVPf5eGyXTZefb/7eO5ZezoAmy5sYtPKVqyMK1+5YPDpboTN3+UMN6L54NYuNuXtM4+0cbhRzPcHDrHxeY+4a9r8o6Kv0Te2TAfEklt7A7Cg74Nhn+82/Uob97hmIQAVu5bZtGjuM7donPusp5zxgI0veurXAHS49YMo9hYfbakppQJFKzWlVKCkvfu56DHXbF1w9n02ricFvq2Sc+edA4/hOTSvoS926U93nAXApc/3s2lbfpLeUZ6MdvzRAFzd0nUNX9/tLrj1j2h+ub0tAPtucrd0/PpO9+F3/1MDG5fPXwwcODJd8Kjbdr+vz7RqpHc6o+hvqev6pJs56Vgbz/rJ30OR+/y+LXMX13Yf5kaKK/aXRn2s/QO+Y+OXT3eniI4sqBv1vhKp1paaiDwhIutF5EtfWgsReVtEFod+5/bYuVIqY0TSUnsKGAs87UsbBUw3xowWkVGhxyNjycDD33e79bec/rapm43XlzaOeH8vzfW+PTq8GtsNtVf2d/X8nWc/a+PKa6H+UTzTpl36bD8bb7nITQHSAQTgoy8AKLngGpuUt2azjQ88+b8WgFUjXett/mnuZPMPH7vK7WO+93vTMHf9234z18b+AYiOz3j3vi2LJf9Zat1I1+I6NM9roe0xLu3yETfYuMH+2XEda+ev3PWBR9d1/7s7zT4bd/rnJgDc1Z/JV2tLzRgzC9h8UPIgYEIongCcn+B8KaVUTGIdKGhtjFkDEPp9aOKypJRSsUv6QIGIlAAlAIW+E5aVxlz0Yxv/vpe7BuzQVxbauHzTwQ3F6nUn/DVpker6qosff8Kt07V2kne91fBmK2yavyvao8R1s4r/kHndz9rKIVnMnC9sXFs3sHCjO8s/bluxjeuu22njb27zTv4/dZnrnlZOswKYu899T2faKh2pKIOS7u9VSfu/hYNt3ODl8F1OyfeqAqlfP+zzfuVHe6cJ7j3iybDP95v7UxsfOm9BrftLtFhbautEpA1A6Pf66jY0xowzxvQxxvQpSNIopqqdlkP6aRmkRqyV2lRgaCgeCkxJTHaUUio+tXY/ReQ5oB/QSkRWArcAo4HJIjIM+BYYXP0eambmzrNxSzeIldLRkupUfO6azk/eey4Aw297OOy2z17qrrG7+Q/HJzdjWWrPIPe5bD7c/elVdjtbfuG6mSVNl9m412vLbXx8PW9b/yjnHF+X8/fDfCOl/C8Buc5+jQv22niXL33/GX1s3OIPywB4vvO/Itjju1VS3veVwSGj09sKrbVSM8ZcXM1T/ROcF6WUiptOk1JKBUrap0mp3LH6IncR6PzTXDfeLRJZdeFIcF1Of7p/lPOyF66zcecZuX03qXGPDbTx1Td6U5ee7uyG9K/+4Cwbj+/oyiAf35zAGFzx6tU27vbhf+PaV7y0paaUChRtqdVg5c1ufbeK3jtq3LZ1nmuFlP3Am6qV/++51W2e88KtpxYu7eD0khU/AGDFb900ulxvnfntaldRJa2+uAnmEzr+2/eMa53dsNYbxHljmltgYn8b9ze95IzHajxuq//FNi0xGbSlppQKFK3UlFKBkhPdz/zOxTZeMqyNjR8aMq7G1/UrdNc5Vd7stTrt8hvZeNyT3jVr13Y8JZpsBl7b5103aHCRO6F9VJPVAFzd0q17VpTnn0bkPvuv/3oEAPVn6J2+wun+6AYbH7F/eI3bdp3oph9WLPwagE5lriv/zeiTqrzG79pVJ9u4xbPuVEs0y4Ang7bUlFKBopWaUipQAtf93Dn4BAA2HOfq69t/NMnGQxpviWJvsdX5A94ZAUB3Po7p9UFVf4rrMu7zzRaeG/qcS/q6lU52/MlN6Pn30c/b+JRbvWugPpvb3qZl2moc6VS+6Gsbdxr1dQ1b1j4VMX93zSOaHz/ey8at9mfOCLS21JRSgaKVmlIqULK2+ym93c1um41dY+M3ir2pH7WNVgK8sssbsfxyT7uwz792Zz8b5+3zxnSG3u6mnJQ0XR32dXXXVr1LVZDkt0/O/Rj8C0o2crN5GPyuGyl9uesbABx1pRtZ7nCrdj+TQarpn5aFOq7NF+0Lv0GaaUtNKRUoWdVSW36bm7b0hyHu5PEljTfZuPK+hgtK3V37fvHclTZusMad/GwzcyMA5V8tCnu8plSdmLv4t619G7iW2tL9bi2w4ik7CaLK9dAqT9YDvLbctZjbnD8/Kcfddpe7v2rFI16LeX+3PUk5lnJ+evG0sOmDl3gt57yZmblenbbUlFKBopWaUipQsqr72ayvu7+Lv8vZ/6vzbLz/gcOAA6+JKib8NTTRLBlecVpvAM5vNt6X6r4TNle4KUCVN/INAv+gwEV/fROAj7cX27RkdTnzmjW18Y9Hu26Qf501lXh5hxxi4271loTdZuPDxQA0Dt2EOtPU2lITkfYiMkNE5ovIPBG5PpTeQkTeFpHFod/Na9uXUkolWyTdzzLgBmPMEcCJwHAR6QmMAqYbY7oB00OPlVIqrSK58coaoPJu7DtEZD5QBAzCu8sUwARgJjAyKbkMaTnMjSp2/bWbUtPlJte9zOfbpBx7S/dCAE4uDP89UPLlpTZuRfjR1Gy0/Cdu5LGkqTe36d5PBti0LnySuIMdf7QNf/jkLHfcZq4bVBH6Hi5YVPtNd1X0tn2/i40HNnDd/p3GXZNWuHF/SvMUragGCkSkGOgNzAZahyq8yorv0ERnTimlohXxQIGINAJeBEYYY7aLRHbCVkRKgBKAQhrUsrVKFi2H9NMySI2IKjURKcCr0J4xxrwUSl4nIm2MMWtEpA2wPtxrjTHjgHEATaRFXOvHla1xoy1dbkrtyMumvmVV0uaX7rZx44eaVnk+k8RaDkUz3L0ZCq731rS/vpdb5378L86xcct5rosS7v4MeT2723h1/1Y2bnSOV5Yzjn7KpvlHOSt8HYrub/7c+32bW1AyWyTyfyFZht42NWz60v2uDAreyex7b0Qy+inAeGC+MeYe31NTgaGheCgw5eDXKqVUqkXSUjsZuAz4QkQ+DaXdDIwGJovIMOBbYHByspg+Z3653cYvN3swFLnr0YbOG2rj5m/OSVW2Ust3zd3Jn/8IOHB9s6tHPWBj/12fblv/nSq7Oq/pczbuXc9tW6eWu0n1eMEtS93z7ysAb0heJV7LvPBT/O5ac6bv0dbUZCZGkYx+vgfVXvHYP7HZUUqp+Og0KaVUoGTVNKlU+3GTz23coI639tqi/W6Z6QZjm6U8T+nU7Crv5ra3TXVdy7+0dp/Rft+p7z8d6p2pqPDdW6i6k//ryr0VNx7a5FZh+ddYd6eibuPddYja7UyP0oq82jfKENpSU0oFilZqSqlA0e7nQdZf67pArfPciGblIpAX/+Umm9bqzcy5g04qVC7d/dlAdyenrn+rOsoJML/f4wCc+vmFNm3D5iZht+06xutU+pfzblnNyioqPR4rfs3G37n7VwB0uaHqIqqZQFtqSqlA0UpNKRUo2v0EpF49G19wtZsCtKOi1MZnf+StCtLhUe0W+W8e3OWS8HdyOhevW9oEd0Pd8J1PyMj5Qjnqd5MusfHhl7sJRIcXuP8RKjJ7oU5tqSmlAkVbagAVrq0w8dXv2/jNz/rZuMPkzDwpqlQidfyj64n8+o8nhd2mS4YP4mhLTSkVKFqpKaUCRbufgNnvBgSKf5fZTWulVM20paaUChSt1JRSgSLGpO4qIRHZAOwCNqbsoKnVisS+t47GmENq3yw6AS+HRJcBJKEcQmWwnOTkNxOkrRxSWqkBiMjHxpg+KT1oimTTe8umvEYj295XtuU3Uul8X9r9VEoFilZqSqlASUelNi4Nx0yVbHpv2ZTXaGTb+8q2/EYqbe8r5efUlFIqmQLf/RSRmSJyZapfqw6k5ZB+uVIGWVOpicgyERmQ7nzUREQ6i8hrIrJDRDaKyJ3pzlOiZXo5iMhRIjIt9PkHshuSBWUgInKHiKwSkW2hCvHIVB0/ayq1TCcidYG3gX8DhwHtgH+kNVO5aT8wGRiW7ozksMHAz4DvAS2AD4GJqTp41ldqItI81DraICJbQnG7gzbrIiIfhb41pohIC9/rTxSRD0Rkq4h8JiL9YszKFcBqY8w9xphdxpi9xpjPa3tRUGRKORhjFhpjxgPz4ng7WSlTygDoBLxnjPnGGFOO9+XeM8Z9RS3rKzW89/Ak0BHoAOwBxh60zeV43xxt8W4deT+AiBQBrwN34H2j3Ai8KCJVrloWkQ6hwu5QTT5OBJaJyJuhrs9METk67neXPTKlHHJZppTBJKCriHQXkQJgKPBWnO8tcsaYrPgBlgEDItiuF7DF93gmMNr3uCdQCuQBI4GJB71+GjDU99orI8zfv/C6Pj8E6gI3Ad8AddP92eVSOfhe39X7807/Z5ZrZRD6+78Pb6X2MmAp0ClVn0/Wt9REpIGIPCoiy0VkOzALaCYi/ltKr/DFy4ECvLlpHYHBoW+drSKyFTgFaBNDVvbgNbnfNMaUAncBLYEjYthX1smgcshZGVQGtwB9gfZAIXAb8G8RaRDDvqKW9ZUacAPQAzjBGNMEODWU7r87RHtf3AGvRbURr4AnGmOa+X4aGmNGx5CPz8nte4hkSjnkskwpg2OB540xK40xZcaYp4DmpOi8WrZVagUiUuj7yQca47WStoZOet4S5nWXikjP0DfF7cALxp3AHCgiZ4pIXmif/cKcXI3EP4ATRWRA6JtxBN4fy/xY3miGy9hyEE8hXheI0L7q1fKybJSxZQDMwWv1tRaROiJyGV6LcElM7zRK2VapvYFXaJU/twJjgPp4Fch/CX9CciLwFLAWrzn8SwBjzApgEHAzsAHv2+omwnwuoZOjO6s7OWqMWQhcCjwCbAnt97xQVzRoMrYc8LpRe3Cjn3uAhVG+v2yQyWXwN+Az4FNgK/Ar4AJjzNbo32b0dJqUUipQsq2lppRSNdJKTSkVKHFVaiJylogsFJElIjIqUZlSSqlYxXxOLTTCtwg4HViJN+JxsTHmq8RlTymlohNPS+14YInx5neV4k2NGJSYbCmlVGziuZlxEQdenbwSOKGmF9SVeqaQhnEcMrfsZRelZp/UvmV0tByik4xy0DKI3g62bDQR3E0qnkotXCFX6cuKSAlQAlBIA06Q/nEcMrfMNtMTti8th9glqhy0DOLzjnlheSTbxdP9XMmBUy7aAasP3sgYM84Y08cY06eAIF7YnR20HNJPyyA14qnU5gDdRKSTeAskDgGmJiZbSikVm5i7n8aYMhG5Dm95kjzgCWNMzi3Mp5TKLPGcU8MY8wbeHDSllMoIOqNAKRUocbXUAqmOW09v0WO9bTzvzIcAGDj0GpuWP31u6vKllIqIttSUUoGilZpSKlC0+wnkd3SX2y36a0sbL+33uG+rugBs7VLXprRK3LWxClg66Rgbv3fywzb+yeW/sHHejP+lNE8q+2hLTSkVKFqpKaUCJae7n/mdiwH46netbNqBXU7nqhUnA9D6PxttWnnyspaTzLdugnfL79W38eYebkrRITNSmqWctu+cvjbefNVOG3/S95kaX3f1yu/Z+L03jwWg86Pf2LSyNWsTlcWwtKWmlAqUnGupSYE70T//1hYALB0QvnXW+Z2f2bhHibf2ZcXexUnMXW5ruDL86j6HXeQWZyh/JFW5yS2V/xeL7nHXZr4+8F4bdy1wreWKWvb1SLv/uG2vmgVAr6Mvt2ntLtCWmlJKRUwrNaVUoORc93Ph2GNtvHTAY1We7zrzCht3u9xdE1Vbk1slz56yAhvXrWE7FbuFD/QCYNHAh2xaHQptXFF1/dcDlKzoZ+PH279b5fn7e02y8d0tT7Nx+abNUee1NtpSU0oFilZqSqlAyYnu55IxJ7r43Id8z3h1eue33Shn9xK3zmVsNw9UsWpyzpqw6dtebGvjQ4homXpVDf/of2WXE2DeuWNDkVulZk35bhuf+vKNNu78cikA9Ra7UczyjZts3Pv5S2w8t+8/APjfnmKbZkr3x5j7yGhLTSkVKFqpKaUCJbDdz9Kz3BSPl88fY+M8cSM6lSOd3X76mU0zFTr5KdXK+x0HwKtHPmjTPi113aDWz3xpYx2Fjs+a4X1svGjgA75nvM97/LYONuWlq063cbf3/1tlX2XVHGPfvoIqaa+uciuw1N+xNMLcxqbWlpqIPCEi60XkS19aCxF5W0QWh343T2oulVIqQpG01J4CxgJP+9JGAdONMaNFZFTo8cjEZy92LX/nvg2OqetaZ6fPH2jj7rdsB6BcW2dpVV7P+25tJG4qzn7jhmkqduxIeZ6C6pqSKTau47sf+V839QTgw/O62zRZ9mmN+8pr0sTGK688ysa/OeYlG39S6rWt65+Z3NaZX60tNWPMLODgK+QGARNC8QTg/ATnSymlYhLrObXWxpg1AMaYNSJyaHUbikgJUAJQSIMYD6fipeWQfloGqZH0gQJjzDhgHEATaZGyS79+WfRO2PTtE9rZuNniD1OVnbRLVzlEYtn/5cYgfCaUQbmvc+af+vTGX/oB0HhZ1QEB4IC7rJWf5k01PHesW8/+6mZuoTt/t/achZWduFWxZjlqsf41rRORNgCh3+sTlyWllIpdrJXaVGBoKB4KTKlhW6WUSplau58i8hzQD2glIiuBW4DRwGQRGQZ8CwxOZiajse1Sb0rUqYVu5Obkz39k42YTq2leq7RpfJiObqZbg7WlNT5f2eUEePMfVVe38fu/JWfbuM4F3lSrVF5fUGulZoy5uJqn+ic4L0opFbfcOEOrlMoZgZsmtfW8XVXSdk89zMaNzDdVno+Zb0QIvYBXZYHFe1q7B02X2fCJp+8HYPS6ATZt5vKuNn7r+Pt9e/Hu9LWtYq9N6fv6r2x8+A1upZuKXVX/H5NNW2pKqUAJXEutTfPtVdLqb4p/GvS+H3oT5Dde5daYOqq1W/9rx4/dOlXJvq9hENQpdFPXTimqOoXmsfWn+R7trPK8is384T3dgxdn27BNntf6uq/t+zatTtsPbFyBuw9rpe8/cJONu9/p3za9tKWmlAoUrdSUUoESiO5n/mHu5OdjPZ4JRY1i2ldes6Y2Pv9Dd+Piixp7J0qb1qnaDAc4cqxbwjjZN2sNgjq+z/mBtm9Wef7d99yqD13Qawvjse8ct7bgiiFuFTT/dKZw8sTX5jGuU9l/nnfdZ1tflzOTaEtNKRUoWqkppQIlEN1PCtzywR3yo+92rr/2uzY+/+czbVzSdLVvq/DdzkqHNE799TjZrKy4dY3Pd3gruXccCqo6xxxu48PGeStjPN7+UZvmX5kj3CjlqLWuq/rSR27p74dPn2Dj8T28O0RdfqG7w1SjyZlzikBbakqpQNFKTSkVKIHofhrfGvbjtnk3vj2w6+jktWpp4xU/6wHAFyMeCrttNLbtcReTVrsMsLI2/m5vlbSzF5xn47ozfXf4SkWGstjGkpNsPO0Pd9m4aZ3Kv8nwo5w3rHE3+X7z315Xs/u97kLo7ms+svFd33ej+5WrdAy5xY1avzY5c+69pC01pVSgBKKlVr51m42fW+md6Cxp6tatPHmkmw7S909uQvuFjdxyxLG4bYObctL2l26goLr7ISrn4aOe8T3yFgZYvd3dnaht2coU5yi77BjiWlnhW2cwf7832HLvWnf/zoVjjnTbvuLWHOy811vavrq/3bx3Xcv58MnDAfhssLuf7stnXGfjgn99HNF7SBZtqSmlAkUrNaVUoASi++m398k2AOz7u7vO6e+HfRL3fvcbb720nu8Os2ndf7vJxmXLV8R9jKDLL+5g48biptjkSUG4zVUNNh7jTv77u5wv72ph4ycvPAeAik+/smmNfVPOollNo059d4wjj1sGQD1fuVXk1zzlKpVqbamJSHsRmSEi80VknohcH0pvISJvi8ji0O/MGf5QSuWsSLqfZcANxpgjgBOB4SLSExgFTDfGdAOmhx4rpVRaRXLjlTVA5d3Yd4jIfKAIGIR3lymACcBMYGRSchmFJs96zevZd7im8amF1W1dVblvNYI+H//ExnVf8BqinSe6GyDrKGd09j7u4u4FrlAqP/NGk5sc/BIVAf9qGyNnXGjj7p/OiWu//ms6G7zsjvF85zdCUeZ0Of2iGigQkWKgNzAbaB2q8CorPr3mVCmVdhEPFIhII+BFYIQxZrtIZLW0iJQAJQCFNIgljyoBtBzST8sgNSKq1ESkAK9Ce8YY81IoeZ2ItDHGrBGRNsD6cK81xowDxgE0kRZpn/Fy+HuX2Vi+bAxAp/vd3W9Muet+HrpjQeoylmTpKoe87l1sfEPx1LDbXLzUuzi0yaTZYZ8PikSWQavP3cu3VOyx8Zyz3QWxfR8dAcARf1xu08rXhf03Jb/Im16469gimzbivudsfE4Dd4F75X/Ig1td2db/z4Iqz6dLJKOfAowH5htj7vE9NRUYGoqHAlMOfq1SSqVaJC21k4HLgC9EpHJexc3AaGCyiAwDvgUGJyeL8ev58LU2Lv6rm6RryrxT/XrHzuQpLXLLdvevvy/sNoue9xYWaG0yc3noTNR4krve7NSu7q5On13zgI0XnfsIAPPOcENaIxZfFHZ/zxzhTVvzX/PmH4Dwt74qJ8Iv+IWbJig7PiNTRDL6+R7VD3P0T2x2lFIqPjpNSikVKIGbJlXpz5172bg9rluT9pEKBcDVK79n47bPLQT0NECsWixwn9wjWzvbuGeht9JJv0LX0Xr7yBer2UvVizkf2dbRxve+fq6Nu/3Bm3YoezOny+mnLTWlVKBopaaUCpTAdj9VZsib8T8bn110nO+ZXdXEKloNX3DX9732gltX4q3iYwG4ZnSzsK/763Gv2PiDHV0BeHXaCTat081uSmAXXJzu69Bqoy01pVSgaKWmlAoU7X4cLigDAAAR6ElEQVQqFVBly74FoNOQb8M+P47Ovkdep7KTr5uZrbSlppQKFK3UlFKBopWaUipQtFJTSgWKVmpKqUDRSk0pFShaqSmlAkWMSd26FSKyAW9OzMaUHTS1WpHY99bRGHNIAvcHBL4cEl0GkIRyCJXBcpKT30yQtnJIaaUGICIfG2P6pPSgKZJN7y2b8hqNbHtf2ZbfSKXzfWn3UykVKFqpKaUCJR2V2rg0HDNVsum9ZVNeo5Ft7yvb8huptL2vlJ9TU0qpZAp891NEZorIlal+rTqQlkNmyIVyyJpKTUSWiciAdOejOiJylIhME5GNIhLY5m+mlwOAiPxKRNaKyDYReUJE6qU7T4mW6eUgIvVE5F4RWS0iW0TkIREpSMWxs6ZSywL7gcnAsHRnJJeJyJnAKLx70hYDnYHb0pmnHDUK6AMcBXQHjgN+n4oDZ32lJiLNReQ1EdkQ+kZ4TUTaHbRZFxH5KPTNPUVEWvhef6KIfCAiW0XkMxHpF0s+jDELjTHjgXlxvJ2slSnlAAwFxhtj5hljtgB/Aq6IcV9ZJ4PKYSBwvzFmszFmA3A/8LMY9xWVrK/U8N7Dk0BHoAOwBxh70DaX432gbYEyvA8YESkCXgfuAFoANwIvikiVq5ZFpEOooDsk6X1ku0wphyMB/w0pPwNai0jLGN9XtsmUcpDQj/9xOxFpGuP7ipwxJit+gGXAgAi26wVs8T2eCYz2Pe4JlAJ5wEhg4kGvnwYM9b32yijz2dX7WNP/meViOQBfA2f5Hhfg3cO6ON2fXY6Vwx3A+8AhwGHA7FA5tEn2Z5P19ygQkQbAvcBZQOX9wRqLSJ4xpvLW1St8L1mO94feCu/bbLCIDPQ9XwDMSG6ugyeDymEn0MT3uDLeEcO+sk4GlcOfgWbAp8A+4DGgN7A+hn1FJQjdzxuAHsAJxpgmwKmhdH/Tt70v7oB3Un8jXuFONMY08/00NMaMTkXGAyZTymEecKzv8bHAOmPMphj2lY0yohyMMXuMMdcZY4qMMZ2BTcBcX8WaNNlWqRWISKHvJx9ojHfeYGvohOctYV53qYj0DH2L3Q68EPpw/wEMFJEzRSQvtM9+YU6s1ko8hUDd0OPCIF5KEJKx5QA8DQwLHac53ojbU7G8ySyQseUgIkUi0jb0f3Ei8Idq8pJw2VapvYFXYJU/twJjgPp43zT/Bd4K87qJeH/Ya4FC4JcAxpgVwCDgZmAD3jfVTYT5XEInRnfWcGK0YyhPlaOfe4CFUb6/bJGx5WCMeQu4E6/LtDz0k5J/pjTI2HIAugAf4C1xNQEYZYz5VwzvMWo6TUopFSjZ1lJTSqkaaaWmlAqUuCo1ETlLRBaKyBIRGZWoTCmlVKxiPqcmInnAIuB0YCUwB7jYGPNV4rKnlFLRiaeldjywxBjzjTGmFJiEN3KilFJpE8+MgiIOvDJ5JXBCTS+oK/VMIQ3jOGRu2csuSs0+qX3L6Gg5RCcZ5aBlEL0dbNloIribVDyVWrhCrtKXFZESoASgkAacIP3jOGRumW2mJ2xfWg6xS1Q5aBnE5x3zwvJItoun+7mSA6dbtANWH7yRMWacMaaPMaZPAUG9wD7zaTmkn5ZBasRTqc0BuolIJxGpCwwBpiYmW0opFZuYu5/GmDIRuQ5vaZI84AljTE4ukKiUyhxxLT1kjHkDb/6ZUkplBJ1RoJQKFK3UlFKBopWaUipQtFJTSgVK1t+jING+vvtEG19/1ps2fuPikwCo+HxByvMUeCceA8DS69313ItOm2DjrjOvsHGXn3yasmyp7KQtNaVUoGilppQKFO1+AvlFbW08dtCTNj69/h4bTzjhbABafp66fAXZ2hHftfFfrnsCgDPq77Jp+32ziO87fpKN7+fwKvta9wu3r7bPutMD5Zs2JySvKrtoS00pFSjaUgO+/nlHG/tbZyp+Us9N3N5y4XE2nnXj3TZuIHWj3u/K37rW2ZzhY2w8ebi7m9v9Yy4A4JBHPox6/7mgzrFH2Hjhr+sDcFmv2TbtFy0+snH/u2+y8WFjPkhB7mKnLTWlVKBopaaUChTtfgLtT16Z7iwE1je3ui7nvMvH+p6pucv5yNbONn504jk2LsLr+uxrWWHTCiTPxpc0XmPjvqPuAeAyfm3TcrEr6j8FsLbkOzaePeo+G++oKAXgxEk32rRZvbra+LRL59h4oevtZyRtqSmlAkUrNaVUoOR093PvuccDcF/nB3ypBenJTMBUdnka9twS8Wve3N3Yxi/+5gwbF70e22hb9wKvizvpt3fZtDN7j3DP/3xOldcESZ3CQgAWjDnGpi0Z6E4BPLC1m43/edtZAHSZ7Lrned272PjzLr1sbAZ609nyd5fbtPzpcxOV7bhpS00pFShaqSmlAqXW7qeIPAGcC6w3xhwVSmsBPA8UA8uAC40xkfczMsSelt6o2dF1tcuZCJLv/py+vt0b9fyqz9jqNrdKVvQDYP0FrvtZb1XNXcPi10ttfEzHK2w896TxNq4cFe2UX2jTmiwIdlnXadDAxque9S4qX9L3EZt2zxbX5Zz2i9Ns3GjGf6vsq3zR1zZusGW7jUd8OBOAx9eeatO2Je5ujnGLpKX2FHDWQWmjgOnGmG7A9NBjpZRKu1pbasaYWSJSfFDyIKBfKJ4AzARGJjBfGeH9fa7Ob7yiLI05yQ77BvS28VeX1txCu371yTZed47XeirfVOW2sdXKm/E/G3eY4dJfXtjGxhc2Wh/x/rKZv3W24O6jbFzZQrtrcw+bNuu8njbOW+o+w9qsuMK18PrXnwbA5kPc659u5gYjyrdui3i/yRDrObXWxpg1AKHfhyYuS0opFbukX9IhIiVACUAhDWrZWiWLlkP6aRmkRqyV2joRaWOMWSMibYBq2/nGmHHAOIAm0sJUt106HH51zfdeHrPydBvXfSu7r2lKVjms+6VbLePaa16pcVt/l3Ppaa6TULE7N9Y9S1YZbLjkWBsvOe9BG7++uxEAswYdadPKli6L6RilTatmd/5etw5hurucfrF2P6cCQ0PxUGBKYrKjlFLxqbVSE5HngA+BHiKyUkSGAaOB00VkMXB66LFSSqVdJKOfF1fzVP8E5yXlrm1dOWwmYZ9f+KYb8WnHhhTkKDv4Fxcc/Ut3XVj/+rurbFt5DRq4UU5IbJdTervuVXFB1RG9Jfv32bjpN8EYxfYvQf+bm5618apyVwZ/veVaAJp8U/UatIiO0bnYxuf+cHb1G2YYnVGglAoUrdSUUoGS06t01KbjK67LWV7DdrnmexNdFy9cl9NvzitH27hoU3LWtl94jbs84vh6VUfppu1yF5zWn/JRleezUUXLJja+oKGboXj7xhNs3OTZyLudlVPcVo043qaNuup5Gw9plD2nX7SlppQKlJxrqfmvq+pRUNlycBOe/SdaKdP2md/Gn58EwDXN7/aluqWi15S7O3H9evn5AHR4aZ1NS+Snmd/J3QHs3bPu9T1Tv8q2723u6nu0MYG5yDznNfnExq+VXA9Awe7wl8RtPseV12vffQiALvmuJfvKrmY27jr1ahsvOc+bfjVnsysDiHyKW7JpS00pFShaqSmlAiUnup95rd18+94/+cLGTeoUVtm238vubjrdFsd2fU9Q7Qj1NhrVqRf2+bvWf99t+73Kbl5yunsLh7vVONrkVe1yAmyp2AvA2vvcstQNA9L9rPhioY27T77WxosufMjGH93yIJF6a09LAM5//Gc2rcOdbonuw3u49dQ4z/u1eI7rfnbW7qdSSiWHVmpKqUDJie4nrZrb8PH2b1V5enuomwLQeKnW87F6650+Nu5Ekm4aLN6UNpNXy3bAjSt/CEDDF7Jnik/EjBvR7Pord5rk+AXDbVxxdtUV9reud0umF7/o0itXoWmPu5bQP2ZqPl9g4zs2egtRXnrmuzbtg9/UfHPqVNL/YKVUoGilppQKlJzofpY3rLlp/MV+N83msDHJmcqTC9q8n/yLlbdd4k0DWnBh7SN7H7zvTY/qQu6MYrd61Nftf7Tq87Guu5/XsoWNezfwPs+5uzvFuLfk0paaUipQcqKl1vjuNTU+f80nl9i4HTUv8a2q1/FmdzJ53avx7Su/XZGNFw/vYOPZl1ZO0Qp/rdxzO1rbuPuT3olynewWP1Pk2njnNNgJwPX/OcmmdefjlOepOtpSU0oFilZqSqlACWz3M799Oxt3b/Rt2G0uWTYAgI5Xuike2lWJ3SnNltj4lW4nAlC++JtaX5d3hLds+uKhrWzamB8/aeMz6u/ybR2+21lpwvBBNs6fN7eGLVU0Vp3eokpa/saCMFumXyQ3XmkvIjNEZL6IzBOR60PpLUTkbRFZHPrdvLZ9KaVUskXS/SwDbjDGHAGcCAwXkZ7AKGC6MaYbMD30WCml0iqSu0mtAdaE4h0iMh8oAgYB/UKbTQBmAiOTkssYrD27vY2nHjrVxnni6vEte73r0+qUuukkUuCuaTP7S5OZxazT7XFvFPm2s3vZtFsO+dTGP22ywsZ5UysA+GK3Ow1QnV4Nvek2lzSueZTab+ou1zG48Z0hNj78v270uiLivana7GueUfchr1FU59REpBjoDcwGWocqPEJ3ag97XZ+IlAAlAIU0CLeJSgEth/TTMkiNiEc/RaQR8CIwwhizvbbtKxljxhlj+hhj+hTUcpJXJY+WQ/ppGaRGRC01ESnAq9CeMca8FEpeJyJtQq20NsD6ZGUykcqN65S8cXioW7rIPd/tBbfgXrfrc2d6TSTKvlkGwLT7T7FpI25zn1FT36KblzdZ5QWVv+Ow27jTAA9u9rq+s37W16Z1/9itq69dThXJ6KcA44H5xph7fE9NBYaG4qHAlMRnTymlohNJS+1k4DLgCxGpPCt8MzAamCwiw4BvgcHJyWJsCje77+yvy9xdc7rkV136eY+vJdBgjV6PXJsWT7hJ03+8pr+Nrz5kpo2PKIjvGqYHt7oluCfe90MbtxpXeewv49q/il3lYFvzDJ1RGMno53uAVPN0/2rSlVIqLbRZopQKlMBOk2r0T7eE84WH3WTjT3/r7rZzx8bDAXhx3A9sWtFYXU8tGl/3dUuhj+p6sUu/4jAAzjzLrd5wdxs3qHDk09fZWMLMTevy7CYbt/oqSUuDq5hUDrY1n78zzTkJT1tqSqlA0UpNKRUoge1++rV+wHUpz3ygV5XnD0W7nIlQvmSpjYt/78ULf++eP5fv2Li2u03paimZyz/VMBNldu6UUipKWqkppQIlJ7qfSqnE+Xq/N+qZt3W3Tcuk0wXaUlNKBYq21JRStSr+vRvYufb3lQsafJ2ezNRCW2pKqUDRSk0pFShaqSmlAkUrNaVUoGilppQKFDEmdXeJEZENwC5gY8oOmlqtSOx762iMOSSB+wMCXw6JLgNIQjmEymA5yclvJkhbOaS0UgMQkY+NMX1SetAUyab3lk15jUa2va9sy2+k0vm+tPuplAoUrdSUUoGSjkptXBqOmSrZ9N6yKa/RyLb3lW35jVTa3lfKz6kppVQyafdTKRUoKa3UROQsEVkoIktEZFQqj51IItJeRGaIyHwRmSci14fSW4jI2yKyOPS7ebrzerCglAFoOWSKjCsHY0xKfoA8vGn9nYG6wGdAz1QdP8HvpQ1wXChuDCwCegJ3AqNC6aOAv6U7r0EtAy2HzPnJtHJIZUvteGCJMeYbY0wpMAkYlMLjJ4wxZo0x5n+heAcwHyjCez8TQptNAM5PTw6rFZgyAC2HTJFp5ZDKSq0IWOF7vDKUltVEpBjoDcwGWhtj1oBX0MCh6ctZWIEsA9ByyBSZUA6prNQkTFpWD72KSCPgRWCEMWZ7uvMTgcCVAWg5ZIpMKYdUVmorgfa+x+2A1Sk8fkKJSAFeAT5jjHkplLxORNqEnm8DrE9X/qoRqDIALYdMkUnlkMpKbQ7QTUQ6iUhdYAgwNYXHTxgREWA8MN8Yc4/vqanA0FA8FJiS6rzVIjBlAFoOmSLTyiHVq3ScDYzBG/15whjz55QdPIFE5BTgP8AXQEUo+Wa88wiTgQ7At8BgY8zmtGSyGkEpA9ByyBSZVg46o0ApFSg6o0ApFShaqSmlAkUrNaVUoGilppQKFK3UlFKBopWaUipQtFJTSgWKVmpKqUD5f1fT8jGEHvVnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Review a few images\n",
    "image_list = mnist.train.images[0:9]\n",
    "image_list_labels = mnist.train.labels[0:9]\n",
    "\n",
    "# https://matplotlib.org/mpl_toolkits/axes_grid/users/overview.html#imagegrid\n",
    "fig = plt.figure(1, (5., 5.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(3, 3),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.3,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for i in range(len(image_list)):\n",
    "    image = image_list[i].reshape(28,28)\n",
    "    grid[i].imshow(image)\n",
    "    grid[i].set_title('Label: {0}'.format(image_list_labels[i].argmax()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark CNN (for testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on challenge requirements, building model using tensorflow low-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To run nicely in jupyter notebook\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for creating weights and biases\n",
    "# https://www.tensorflow.org/get_started/mnist/pros\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Functions for convolution and pooling functions\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pooling_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create placeholders nodes for images and label inputs\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark model to be used for testing classification on real images with and without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y = (Wx +b)\n",
    "# https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "# Input layer\n",
    "x_image = tf.reshape(x, [-1,28,28,1]) # mnist image comes in as 784 vector\n",
    "\n",
    "# Conv layer 1 - 32x5x5\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "x_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "x_pool1 = max_pooling_2x2(x_conv1)\n",
    "\n",
    "# Conv layer 2 - 64x5x5\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "x_conv2 = tf.nn.relu(conv2d(x_pool1, W_conv2) + b_conv2)\n",
    "x_pool2 = max_pooling_2x2(x_conv2)\n",
    "\n",
    "# Flatten - keras 'flatten'\n",
    "x_flat = tf.reshape(x_pool2, [-1, 7*7*64])\n",
    "\n",
    "# Dense fully connected layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024]) # max pooling reduced image to 7x7\n",
    "b_fc1 = bias_variable([1024])\n",
    "x_fc1 = tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Regularization with dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x_fc1_drop = tf.nn.dropout(x_fc1, keep_prob)\n",
    "\n",
    "# Classification layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.matmul(x_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Probabilities - output from model (not the same as logits)\n",
    "y = tf.nn.softmax(y_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-dd05cd402889>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup to test accuracy of model\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initilize all global variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200, training accuracy 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400, training accuracy 0.92\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "# Run once to get the model to a good confidence level\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fadf5bbdc965>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(batch.shape)\n",
    "print(batch[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5fe27ac8c29f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run trained model against test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images[0:500], \n\u001b[1;32m----> 3\u001b[1;33m                                                   y_: mnist.test.labels[0:500], keep_prob: 1.0}))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     \"\"\"\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4935\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4936\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4937\u001b[1;33m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[0;32m   4938\u001b[0m                        \u001b[1;34m\"session is registered. Use `with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4939\u001b[0m                        \u001b[1;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Run trained model against test data\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images[0:500], \n",
    "                                                  y_: mnist.test.labels[0:500], keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_predictions(image_list):\n",
    "    prob = y.eval(feed_dict={x: image_list, keep_prob: 1.0})\n",
    "    \n",
    "    pct_list = np.zeros(len(image_list))\n",
    "    pred_list = np.argmax(prob, axis=1)\n",
    "    for i in range(len(prob)):\n",
    "        pct_list[i] = prob[i][pred_list[i]]\n",
    "        \n",
    "    return pred_list, pct_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_predictions(image_list, output_probs=False, adversarial=False):\n",
    "    '''\n",
    "    Evaluate images against trained model and plot images.\n",
    "    If adversarial == True, replace middle image title appropriately\n",
    "    Return probability list if output_probs == True\n",
    "    '''\n",
    "    prob = y.eval(feed_dict={x: image_list, keep_prob: 1.0})\n",
    "    \n",
    "    pred_list = np.zeros(len(image_list)).astype(int)\n",
    "    pct_list = np.zeros(len(image_list)).astype(int)\n",
    "    \n",
    "    # Setup image grid\n",
    "    import math\n",
    "    cols = 3\n",
    "    rows = math.ceil(image_list.shape[0]/cols)\n",
    "    fig = plt.figure(1, (12., 12.))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(rows, cols),  # creates grid of axes\n",
    "                     axes_pad=0.5,  # pad between axes in inch.\n",
    "                     )\n",
    "    \n",
    "    # Get probs, images and populate grid\n",
    "    for i in range(len(prob)):\n",
    "        pred_list[i] = np.argmax(prob[i]) # for mnist index == classification\n",
    "        pct_list[i] = prob[i][pred_list[i]] * 100\n",
    "\n",
    "        image = image_list[i].reshape(28,28)\n",
    "        grid[i].imshow(image)\n",
    "        \n",
    "        grid[i].set_title('Label: {0} \\nCertainty: {1}%' \\\n",
    "                          .format(pred_list[i], \n",
    "                                  pct_list[i]))\n",
    "        \n",
    "        # Only use when plotting original, partial deriv and adversarial images\n",
    "        if (adversarial) & (i % 3 == 1): \n",
    "            grid[i].set_title(\"Adversarial \\nPartial Derivatives\")\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    return prob if output_probs else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get 10 2s [:,2] from top 500 [0:500], nonzero returns tuple, get index[0], then first 10 [0:10]\n",
    "index_of_2s = np.nonzero(mnist.test.labels[0:500][:,2])[0][0:10]\n",
    "x_batch = mnist.test.images[index_of_2s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0f09f14b048a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-11dbc291a14f>\u001b[0m in \u001b[0;36mplot_predictions\u001b[1;34m(image_list, output_probs, adversarial)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mlist\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_probs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     '''\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpred_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     \"\"\"\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4935\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4936\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4937\u001b[1;33m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[0;32m   4938\u001b[0m                        \u001b[1;34m\"session is registered. Use `with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4939\u001b[0m                        \u001b[1;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "plot_predictions(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All above 2s are correctly classified, although at varying degrees of certainty (mostly above 95%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand on benchmark model using low-level API. Create adversarial image of a 2 to be misclassified as a 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mostly inspired by:\n",
    "# https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture\n",
    "def create_plot_adversarial_images(x_image, y_label, lr=0.1, n_steps=1, output_probs=False):\n",
    "    \n",
    "    original_image = x_image\n",
    "    probs_per_step = []\n",
    "    \n",
    "    # Calculate loss, derivative and create adversarial image\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/train/gradient_computation\n",
    "    loss =  tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_conv)\n",
    "    deriv = tf.gradients(loss, x)\n",
    "    image_adv = tf.stop_gradient(x - tf.sign(deriv)*lr/n_steps)\n",
    "    image_adv = tf.clip_by_value(image_adv, 0, 1) # prevents -ve values creating 'real' image\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Calculate derivative and adversarial image\n",
    "        dydx = sess.run(deriv, {x: x_image, keep_prob: 1.0}) # can't seem to access 'deriv' w/o running this\n",
    "        x_adv = sess.run(image_adv, {x: x_image, keep_prob: 1.0})\n",
    "        \n",
    "        # Create darray of 3 images - orig, noise/delta, adversarial\n",
    "        x_image = np.reshape(x_adv, (1, 784))\n",
    "        img_adv_list = original_image\n",
    "        img_adv_list = np.append(img_adv_list, dydx[0], axis=0)\n",
    "        img_adv_list = np.append(img_adv_list, x_image, axis=0)\n",
    "\n",
    "        # Print/plot images and return probabilities\n",
    "        probs = plot_predictions(img_adv_list, output_probs=output_probs, adversarial=True)\n",
    "        probs_per_step.append(probs) if output_probs else None\n",
    "    \n",
    "    return probs_per_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_adversarial_image(x_image, y_label, lr=0.1, n_steps=1):\n",
    "    \n",
    "    original_image = x_image\n",
    "    probs_per_step = []\n",
    "    \n",
    "    # Calculate loss, derivative and create adversarial image\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/train/gradient_computation\n",
    "    loss =  tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_conv)\n",
    "    deriv = tf.gradients(loss, x)\n",
    "    image_adv = tf.stop_gradient(x - tf.sign(deriv)*lr/n_steps)\n",
    "    image_adv = tf.clip_by_value(image_adv, 0, 1) # prevents -ve values creating 'real' image\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # Calculate derivative and adversarial image\n",
    "        dydx = sess.run(deriv, {x: x_image, keep_prob: 1.0}) # can't seem to access 'deriv' w/o running this\n",
    "        x_adv = sess.run(image_adv, {x: x_image, keep_prob: 1.0})\n",
    "        x_image = np.reshape(x_adv, (1, 784))\n",
    "        \n",
    "    label, prob = create_predictions(x_image)\n",
    "    return x_image, label, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pick a random 2 image from first 1000 images \n",
    "# Create adversarial image and with target label 6\n",
    "index_of_2s = np.nonzero(mnist.test.labels[0:1000][:,2])[0]\n",
    "rand_index = np.random.randint(0, len(index_of_2s))\n",
    "image_norm = mnist.test.images[index_of_2s[rand_index]]\n",
    "image_norm = np.reshape(image_norm, (1, 784))\n",
    "label_adv = [0,0,0,0,0,0,1,0,0,0] # one hot encoded, adversarial label 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_specific_adversarial_image(originNumber, destinationNumber, n_steps=10):\n",
    "    index_of_number = np.nonzero(mnist.test.labels[0:1000][:,originNumber])[0]\n",
    "    rand_index = np.random.randint(0, len(index_of_number))\n",
    "    image_norm = mnist.test.images[index_of_number[rand_index]]\n",
    "    image_norm = np.reshape(image_norm, (1, 784))    \n",
    "    label_adv = np.zeros(10).astype(int) # one hot encoded\n",
    "    label_adv[destinationNumber] = 1;\n",
    "    adv_image, label, prob = create_adversarial_image(image_norm, label_adv, lr=0.2, n_steps=n_steps)\n",
    "    return adv_image, label, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_trainSet():\n",
    "    number_of_examples_per_number = 1;\n",
    "    original_number = np.zeros(shape=(1, 1))\n",
    "    original_number = np.delete(original_number, 0, 0)\n",
    "    target_number = np.zeros(shape=(1, 10))\n",
    "    target_number = np.delete(target_number, 0, 0)\n",
    "    prob_list = np.zeros(shape=(1, 1))\n",
    "    prob_list = np.delete(prob_list, 0, 0)\n",
    "    adv_img_list = np.zeros(shape=(1, 784))\n",
    "    adv_img_list = np.delete(adv_img_list, 0, 0)\n",
    "    for origin in range(3):\n",
    "        for p in range(number_of_examples_per_number):\n",
    "            for target in range(3):\n",
    "                if origin != target:\n",
    "                    prob = 0\n",
    "                    label = origin\n",
    "                    n_steps = 10\n",
    "                    while prob < 0.9 or label != target:\n",
    "                        adv_image, label, prob = create_specific_adversarial_image(origin, target, n_steps)\n",
    "                        prob = prob[0]\n",
    "                        label = label[0]\n",
    "                        print(\"-\", end=\"\", flush=True)\n",
    "                        \n",
    "                        #print((origin, target, label, prob, n_steps), end=\"\", flush=True)                        \n",
    "                        #print(\" X \")\n",
    "                        n_steps = n_steps + 2\n",
    "                    original_number = np.append(original_number, origin)\n",
    "                    target_to_add = np.zeros(shape=(1,10))\n",
    "                    target_to_add[0][target] = 1 #to vectorize the labeling\n",
    "                    target_number = np.concatenate((target_number, target_to_add), axis=0)\n",
    "                    prob_list = np.append(prob_list, prob)\n",
    "                    #the action of concation can be i,proved\n",
    "                    adv_img_list = np.concatenate((adv_img_list, adv_image), axis=0)\n",
    "                    print(\"|\", end=\"\", flush=True)\n",
    "        print(\"Number\")            \n",
    "        #print(original_number, target_number, prob_list)\n",
    "    return original_number, target_number, prob_list, adv_img_list\n",
    "\n",
    "\n",
    "def generate_adversarial_and_save():\n",
    "    original_number, target_number, prob_list, adv_img_list = generate_trainSet()\n",
    "    np.save('original_number.npy', original_number)\n",
    "    np.save('target_number.npy', target_number)\n",
    "    np.save('prob_list.npy', prob_list)\n",
    "    np.save('adv_img_list.npy', adv_img_list)\n",
    "    \n",
    "    \n",
    "def load_saved_adversarial():    \n",
    "    original_number = np.load('original_number.npy')\n",
    "    target_number = np.load('target_number.npy')\n",
    "    prob_list = np.load('prob_list.npy')\n",
    "    adv_img_list = np.load('adv_img_list.npy')\n",
    "    return original_number, target_number, prob_list, adv_img_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-edae0048ef66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moriginal_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv_img_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_trainSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-d647fdbe78ab>\u001b[0m in \u001b[0;36mgenerate_trainSet\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mn_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[1;32mwhile\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.9\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                         \u001b[0madv_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_specific_adversarial_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-1e009adbae9b>\u001b[0m in \u001b[0;36mcreate_specific_adversarial_image\u001b[1;34m(originNumber, destinationNumber, n_steps)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlabel_adv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# one hot encoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mlabel_adv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdestinationNumber\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0madv_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_adversarial_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_adv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0madv_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-3b91dd0df581>\u001b[0m in \u001b[0;36mcreate_adversarial_image\u001b[1;34m(x_image, y_label, lr, n_steps)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Calculate derivative and adversarial image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mdydx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mderiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# can't seem to access 'deriv' w/o running this\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mx_adv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_adv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mx_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_adv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sess' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "original_number, target_number, prob_list, adv_img_list = generate_trainSet()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adv_img_list' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-bcf7bd784359>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# uniting and shuffeling between adv inputs and benign\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# creating labels as 2d array which [0,1] is adv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnumber_of_adv_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madv_img_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0madv_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_adv_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0madv_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0madv_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_adv_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'adv_img_list' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# uniting and shuffeling between adv inputs and benign\n",
    "# creating labels as 2d array which [0,1] is adv\n",
    "number_of_adv_images = adv_img_list.shape[0]\n",
    "adv_label = np.zeros(number_of_adv_images)\n",
    "adv_label = np.c_[adv_label, np.ones(number_of_adv_images)]\n",
    "\n",
    "number_of_benign = 1000\n",
    "benign_label = np.ones(number_of_benign)\n",
    "benign_label = np.c_[benign_label, np.zeros(number_of_benign)]\n",
    "# uniting adv and benign\n",
    "united_labels = np.concatenate((adv_label, benign_label), axis=0)\n",
    "united_images = np.concatenate((adv_img_list, mnist.train.images[0:number_of_benign]), axis=0)\n",
    "united_images_and_labels = np.c_[united_images, united_labels]\n",
    "np.random.shuffle(united_images_and_labels)\n",
    "#add precondition\n",
    "images_data, labels_data = np.hsplit(united_images_and_labels, [united_images_and_labels.shape[1]-2])\n",
    "train_size = 700\n",
    "image_train_data, image_test_data = np.split(images_data, [train_size])\n",
    "label_train_data, label_test_data = np.split(labels_data, [train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_train_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-dd3f5464986c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_train_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'image_train_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "print(image_train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders nodes for images and label inputs\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='x')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2], name='y_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified nn based on original\n",
    "\n",
    "# Input layer\n",
    "x_image_new = tf.reshape(x, [-1,28,28,1]) # mnist image comes in as 784 vector\n",
    "\n",
    "# Conv layer 1 - 32x5x5 - using same weights and biases as the pretrained\n",
    "# W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# b_conv1 = bias_variable([32])\n",
    "x_conv1 = tf.nn.relu(conv2d(x_image_new, W_conv1) + b_conv1)\n",
    "x_pool1 = max_pooling_2x2(x_conv1)\n",
    "\n",
    "# Conv detector layer 1  - 32x5x5\n",
    "W_conv1_detector = weight_variable([5, 5, 32, 32])\n",
    "b_conv1_detector = bias_variable([32])\n",
    "x_conv1_detector = tf.nn.relu(conv2d(x_pool1, W_conv1_detector) + b_conv1_detector)\n",
    "x_pool1_detector = max_pooling_2x2(x_conv1_detector)\n",
    "\n",
    "# Conv layer 2 - 64x5x5\n",
    "# W_conv2 = weight_variable([5, 5, 96, 64])\n",
    "# b_conv2 = bias_variable([64])\n",
    "x_conv2 = tf.nn.relu(conv2d(x_pool1, W_conv2) + b_conv2)\n",
    "x_pool2 = max_pooling_2x2(x_conv2)\n",
    "\n",
    "# stacking conv2 output and x_pool1 detector output\n",
    "conv_detector_2_input = tf.concat([x_conv2, x_pool1], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conv detector layer 2 - 64x5x5\n",
    "W_conv2_detector = weight_variable([5, 5, 96, 64])\n",
    "b_conv2_detector = bias_variable([64])\n",
    "x_conv2_detector = tf.nn.relu(conv2d(conv_detector_2_input, W_conv2_detector) + b_conv2_detector)\n",
    "x_pool2_detector = max_pooling_2x2(x_conv2_detector)\n",
    "# Flatten - keras 'flatten'\n",
    "pool_shape = x_pool2_detector.shape\n",
    "shape_size = (pool_shape[1]*pool_shape[2]*pool_shape[3]).value\n",
    "x_flat_detector = tf.reshape(x_pool2_detector, [-1, shape_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_fc1_detector = weight_variable([shape_size, 1024]) # max pooling reduced image to 7x7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_fc1_detector = bias_variable([1024])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_fc1_detector = tf.nn.relu(tf.matmul(x_flat_detector, W_fc1_detector) + b_fc1_detector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Regularization with dropout\n",
    "keep_prob_detector = tf.placeholder(tf.float32)\n",
    "x_fc1_drop_detector = tf.nn.dropout(x_fc1, keep_prob_detector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Classification layer\n",
    "W_fc2_detector = weight_variable([1024, 2])\n",
    "b_fc2_detector = bias_variable([2])\n",
    "y_conv= tf.matmul(x_fc1_drop_detector, W_fc2_detector) + b_fc2_detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities - output from model (not the same as logits)\n",
    "# CHANGE!!-----------------------------------------------------------------\n",
    "y = tf.nn.softmax(y_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "# add variables to optimize here:\n",
    "var_list = [W_conv1_detector, W_conv2_detector, W_fc2_detector]\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy, var_list=var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to test accuracy of model\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-a1509e4bdec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sess' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "sess.close()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-fae19a2576a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Initilize all global variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# might want to remove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sess' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Initilize all global variables\n",
    "# might want to remove\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_train_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-1bee6577e769>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mimage_train_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"reached limit of train data current index: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data size: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_train_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Train model\n",
    "# Run once to get the model to a good confidence level\n",
    "batch_size = 100\n",
    "batch_index = 0\n",
    "for i in range(100):\n",
    "    if(batch_size>=image_train_data.shape[0]):\n",
    "        print(\"reached limit of train data current index: \", batch_index, \"data size: \", train_data.shape[0])\n",
    "        break\n",
    "    batch_image = image_train_data[batch_index: (batch_index + batch_size)]\n",
    "    batch_label = label_train_data[batch_index: (batch_index + batch_size)]\n",
    "    if i%20 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch_image, y_: batch_label, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "    train_step.run(feed_dict={x: batch_image, y_: batch_label, keep_prob: 0.4})\n",
    "    batch_index = batch_index + batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_train_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-feffda0c572a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_image\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_train_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "batch_image = image_train_data[0: 100]\n",
    "print(batch_image[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-68a4b72878c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run trained model against test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: test_data[0], \n\u001b[0m\u001b[0;32m      3\u001b[0m                                                   y_: test_data[1], keep_prob: 1.0}))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Run trained model against test data\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: test_data[0], \n",
    "                                                  y_: test_data[1], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
