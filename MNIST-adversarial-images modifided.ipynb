{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies for entire notebook here\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for creating weights and biases\n",
    "# https://www.tensorflow.org/get_started/mnist/pros\n",
    "def weight_variable(shape, name=None):    \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "# Functions for convolution and pooling functions\n",
    "def conv2d(x, W, name=None):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME', name=name)\n",
    "\n",
    "def max_pooling_2x2(x, name=None):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "def load_saved_adversarial():    \n",
    "    local_path = \"C:\\\\Users\\\\alonh\\Documents\\\\Thesis\\\\MNIST-adversarial-images\\\\\"\n",
    "    original_number = np.load(local_path + \"original_number.npy\")\n",
    "    target_number = np.load(local_path + \"target_number.npy\")\n",
    "    prob_list = np.load(local_path + \"prob_list.npy\")\n",
    "    adv_img_list = np.load(local_path + \"adv_img_list.npy\")\n",
    "    return original_number, target_number, prob_list, adv_img_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_adversarial\n",
    "original_number, target_number, prob_list, adv_img_list = load_saved_adversarial()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniting and shuffeling between adv inputs and benign\n",
    "# # creating [0, 1] labels for adv\n",
    "number_of_adv_images = adv_img_list.shape[0]\n",
    "adv_label = np.zeros(number_of_adv_images)\n",
    "adv_label = np.c_[adv_label, np.ones(number_of_adv_images)]\n",
    "\n",
    "# creating [1,0] labels for benign\n",
    "number_of_benign = 1000\n",
    "benign_label = np.ones(number_of_benign)\n",
    "benign_label = np.c_[benign_label, np.zeros(number_of_benign)]\n",
    "# uniting adv and benign\n",
    "united_labels = np.concatenate((adv_label, benign_label), axis=0)\n",
    "united_images = np.concatenate((adv_img_list, mnist.train.images[0:number_of_benign]), axis=0)\n",
    "united_images_and_labels = np.c_[united_images, united_labels]\n",
    "np.random.shuffle(united_images_and_labels)\n",
    "#add precondition\n",
    "images_data, labels_data = np.hsplit(united_images_and_labels, [united_images_and_labels.shape[1]-2])\n",
    "train_test_rate = 0.7\n",
    "train_size = int(images_data.shape[0]*train_test_rate)\n",
    "image_train_data, image_test_data = np.split(images_data, [train_size])\n",
    "label_train_data, label_test_data = np.split(labels_data, [train_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\alonh\\Documents\\Thesis\\MNIST-adversarial-images\\original-nn-data.ckpt\n"
     ]
    }
   ],
   "source": [
    "# should start run from here\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "W_conv1 = tf.get_variable(\"W_conv1\", shape=[5, 5, 1, 32])\n",
    "b_conv1 = tf.get_variable(\"b_conv1\", shape=[32])\n",
    "W_conv2 = tf.get_variable(\"W_conv2\", shape=[5, 5, 32, 64])\n",
    "b_conv2 = tf.get_variable(\"b_conv2\", shape=[64])\n",
    "saver = tf.train.Saver(var_list={'W_conv1': W_conv1, 'b_conv1': b_conv1, 'W_conv2': W_conv2, 'b_conv2': b_conv2})\n",
    "saver.restore(sess, \"C:\\\\Users\\\\alonh\\Documents\\\\Thesis\\\\MNIST-adversarial-images\\\\original-nn-data.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders nodes for images and label inputs\n",
    "# Create placeholders nodes for images and label inputs\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='xDetector')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2], name='yDetector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified nn based on original\n",
    "# Input layer\n",
    "x_image_new = tf.reshape(x, [-1,28,28,1]) # mnist image comes in as 784 vector\n",
    "\n",
    "# # Conv layer 1 - 32x5x5 - using same weights and biases as the pretrained\n",
    "# W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# b_conv1 = bias_variable([32])\n",
    "x_conv1 = tf.nn.relu(conv2d(x_image_new, W_conv1) + b_conv1, name=\"x_conv1_new\")\n",
    "x_pool1 = max_pooling_2x2(x_conv1, name=\"x_pool1_new\")\n",
    "# Conv detector layer 1  - 32x5x5\n",
    "W_conv1_detector = weight_variable([5, 5, 32, 32], name=\"W_conv1_detector\")\n",
    "b_conv1_detector = bias_variable([32], name=\"b_conv1_detector\")\n",
    "x_conv1_detector = tf.nn.relu(conv2d(x_pool1, W_conv1_detector) + b_conv1_detector, name=\"x_conv1_detector\")\n",
    "x_pool1_detector = max_pooling_2x2(x_conv1_detector, name=\"x_pool1_detector\")\n",
    "\n",
    "# # Conv layer 2 - 64x5x5\n",
    "# W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "# b_conv2 = bias_variable([64])\n",
    "x_conv2 = tf.nn.relu(conv2d(x_pool1, W_conv2) + b_conv2, name=\"x_conv2_new\")\n",
    "x_pool2 = max_pooling_2x2(x_conv2, name=\"x_pool2_new\")\n",
    "\n",
    "# stacking conv2 output and x_pool1 detector output\n",
    "conv_detector_2_input = tf.concat([x_conv2, x_pool1], 3)\n",
    "\n",
    "\n",
    "# Conv detector layer 2 - 64x5x5\n",
    "W_conv2_detector = weight_variable([5, 5, 96, 64], name=\"W_conv2_detector\")\n",
    "b_conv2_detector = bias_variable([64], name=\"b_conv2_detector\")\n",
    "x_conv2_detector = tf.nn.relu(conv2d(conv_detector_2_input, W_conv2_detector) + b_conv2_detector, name=\"x_conv2_detector\")\n",
    "x_pool2_detector = max_pooling_2x2(x_conv2_detector, name=\"x_pool2_detector\")\n",
    "# Flatten - keras 'flatten'\n",
    "pool_shape = x_pool2_detector.shape\n",
    "shape_size = (pool_shape[1]*pool_shape[2]*pool_shape[3]).value\n",
    "x_flat_detector = tf.reshape(x_pool2_detector, [-1, shape_size], name=\"x_flat_detector\")\n",
    "\n",
    "\n",
    "W_fc1_detector = weight_variable([shape_size, 1024], name=\"W_fc1_detector\") # max pooling reduced image to 7x7\n",
    "b_fc1_detector = bias_variable([1024], name=\"b_fc1_detector\")\n",
    "x_fc1_detector = tf.nn.relu(tf.matmul(x_flat_detector, W_fc1_detector) + b_fc1_detector, name=\"x_fc1_detector\")\n",
    "\n",
    "# Regularization with dropout\n",
    "keep_prob_detector = tf.placeholder(tf.float32, name=\"keep_prob_detector\")\n",
    "x_fc1_drop_detector = tf.nn.dropout(x_fc1_detector, keep_prob_detector, name=\"x_fc1_drop_detector\")\n",
    "\n",
    "\n",
    "# Classification layer\n",
    "W_fc2_detector = weight_variable([1024, 2], name=\"W_fc2_detector\")\n",
    "b_fc2_detector = bias_variable([2], name=\"b_fc2_detector\")\n",
    "y_conv= tf.matmul(x_fc1_drop_detector, W_fc2_detector) + b_fc2_detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities - output from model (not the same as logits)\n",
    "# CHANGE!!-----------------------------------------------------------------\n",
    "y = tf.nn.softmax(y_conv, name=\"y_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cross_entropy2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv), name=\"cross_entropy2\")\n",
    "# add variables to optimize here:\n",
    "var_list = [W_conv1_detector, W_conv2_detector, W_fc2_detector, b_conv1_detector, b_conv2_detector, b_fc2_detector]\n",
    "train_step2 = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy2, var_list=var_list, name=\"train_step2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to test accuracy of model\n",
    "correct_prediction2 = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1), name=\"correct_prediction2\")\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32), name=\"accuracy2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized_global_variables(sess):\n",
    "    \"\"\"\n",
    "    Only initializes the variables of a TensorFlow session that were not\n",
    "    already initialized.\n",
    "    :param sess: the TensorFlow session\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # List all global variables\n",
    "    global_vars = tf.global_variables()\n",
    "\n",
    "    # Find initialized status for all variables\n",
    "    is_var_init = [tf.is_variable_initialized(var) for var in global_vars]\n",
    "    is_initialized = sess.run(is_var_init)\n",
    "\n",
    "    # List all variables that were not initialized previously\n",
    "    not_initialized_vars = [var for (var, init) in\n",
    "                            zip(global_vars, is_initialized) if not init]\n",
    "    # for uninit in not_initialized_vars:\n",
    "    #     print(uninit)\n",
    "    # Initialize all uninitialized variables found, if any\n",
    "    if len(not_initialized_vars):        \n",
    "        sess.run(tf.variables_initializer(not_initialized_vars)) \n",
    "\n",
    "initialize_uninitialized_global_variables(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.55\nstep 2, training accuracy 0.57\nstep 4, training accuracy 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6, training accuracy 0.55\nstep 8, training accuracy 0.49\nstep 10, training accuracy 0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12, training accuracy 0.62\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "# Run once to get the model to a good confidence level\n",
    "batch_size = 100\n",
    "batch_index = 0\n",
    "image_train_size = image_train_data.shape[0]\n",
    "number_of_steps = int(image_train_size / batch_size)\n",
    "for i in range(number_of_steps):\n",
    "    # if(batch_index>=image_train_data.shape[0]):\n",
    "    #     print(\"reached limit of train data current index: \", batch_index, \"data size: \", image_train_data.shape[0])\n",
    "    #     break\n",
    "    batch_image = image_train_data[batch_index: (batch_index + batch_size)]\n",
    "    batch_label = label_train_data[batch_index: (batch_index + batch_size)]\n",
    "    if i % 2 == 0:\n",
    "        train_accuracy = accuracy2.eval(session=sess, feed_dict={x: batch_image, y_:batch_label, keep_prob_detector: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "    train_step2.run(feed_dict={x: batch_image, y_: batch_label, keep_prob_detector: 0.4})\n",
    "    batch_index = batch_index + batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.619298\n"
     ]
    }
   ],
   "source": [
    "# Run trained model against test data\n",
    "print(\"test accuracy %g\"%accuracy2.eval(session=sess, feed_dict={x: image_test_data, \n",
    "                                                  y_: label_test_data, keep_prob_detector: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
