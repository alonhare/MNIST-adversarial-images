{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Adversarial Images with TensorFlow\n",
    "\n",
    "Create adversarial images to fool a MNIST classifier in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tech Challenge & Benchmark\n",
    "\n",
    "**Backstory**\n",
    "\n",
    "The original concept of this notebook was based on a Machine Learning (intern) candidate tech challenge from the Toronto startup [500px](https://500px.com).\n",
    "\n",
    "When I first saw the posting, it was at the beginning of my 3 month career pivot into Deep Learning and I thought this challenge would be a great way for me to benchmark my progress once I get started. You can read more about my career transition journey on [Medium](https://medium.com/towards-data-science/my-3-month-deep-learning-career-pivot-af94cd8d6a31) and a revised/updated version on [LinkedIn]().\n",
    "\n",
    "Although, I didn't follow through with providing the entire final output of the challenge, I'm quite satisfied that I've successfully completed it and consider it a demonstration of my current knowledge and capability.\n",
    "\n",
    "Prior to starting this challenge, I completed [Fast.ai: Practical Deep Learning - Part 1](http://course.fast.ai/). Read through my blog post to see my reading material - [Deep Learning Reading List](http://jasonicarter.github.io/deep-learning-reading-list).\n",
    "\n",
    "**The Challenge (summarized)**\n",
    "\n",
    "Create adversarial images to fool a MNIST classifier in TensorFlow.\n",
    "1. Learn how adversarial examples are created. For example, “Breaking Linear Classifiers on ImageNet” gives a good overview on the subject.\n",
    "2. Install Tensorflow\n",
    "3. Follow “Deep MNIST for Experts” tutorial to get the MNIST classifier running.\n",
    "4. Expand the code from the previous step to generate adversarial images. Specifically, pick 10 images of digit ‘2’ which are correctly classified as ‘2’ by the trained model and modify them so the network incorrectly classifies them as 6.\n",
    "5. Generate adversarial examples and save them as a single image containing a grid of 10 rows and 3 columns. The rows correspond to the selected examples of ‘2’. The columns are original image, delta and adversarial image. Provide link to the resulting image.\n",
    "6. Make your code clean and readable. Add comments where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "1. Read basic need-to-know about adversarial images\n",
    "2. Get data to be used throughout notebook\n",
    "3. Build a simple CNN (test model)\n",
    "4. Train it on MNIST \n",
    "5. Show model classification of 10 handwritten 2s\n",
    "6. Create adversarial image to classify 2s as 6s\n",
    "7. Test adversarial image with original model (in step 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies for entire notebook here\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.images shape:  (55000, 784)\nTraining.labels shape:  (55000, 10)\nShape of an image:  (784,)\nExample label:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Take a look the training data\n",
    "print('Training.images shape: ', mnist.train.images.shape)\n",
    "print('Training.labels shape: ', mnist.train.labels.shape)\n",
    "print('Shape of an image: ', mnist.train.images[0].shape)\n",
    "print('Example label: ', mnist.train.labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE/CAYAAAAnhFRiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XeYVcX5wPHvy+7C0qsiLGXpig0MWKJRErBERfzFoBgLJuhGxUQSNRBTLDEJMRZUbCgqEhWJDaxECUgsQSRWpCogvfe27O78/jh3Zw7s3d3by7nv53n22ffOPfecuXd2586cOTNHjDEopVRQ1El3BpRSKpG0UlNKBYpWakqpQNFKTSkVKFqpKaUCRSs1pVSgBL5SE5GZInJlql+rDqTlkBlyoRyyplITkWUiMiDd+aiOiDwiIjt9P/tEZEe685VoWVAOQ0RkoYhsE5H1IjJBRJqkO1+Jlunl4Cci/xYRIyL5qThe1lRqmc4Yc7UxplHlD/Ac8M905ysHvQ+cbIxpCnQG8oE70pul3CUil+CVQcpkfaUmIs1F5DUR2SAiW0Jxu4M26yIiH4W+vaeISAvf608UkQ9EZKuIfCYi/RKQp4bABcCEePeVLTKlHIwxK4wxG31J5UDXWPaVjTKlHEL7agrcAvwm1n3EIusrNbz38CTQEegA7AHGHrTN5cDPgLZAGXA/gIgUAa/jfZO3AG4EXhSRQw4+iIh0CBV0hwjydAGwAZgVyxvKUhlTDiJyiohsA3bglcWY+N5aVsmYcgD+AjwMrI3nDUXNGJMVP8AyYEAE2/UCtvgezwRG+x73BEqBPGAkMPGg108Dhvpee2UMeZ0O3Jruz0zLgSLgVqB7uj+3XCsHoA/wKV7XsxgwQH4qPpusb6mJSAMReVRElovIdrzWUTMRyfNttsIXLwcKgFZ432aDQ984W0VkK3AK0CaO/LQHTgOejnUf2SjTygHAGLMKeAuYFM9+skkmlIOI1AEeAq43xpTF835ikdITeElyA9ADOMEYs1ZEegGfAOLbpr0v7gDsBzbiFe5EY8xVCczP5cAHxphvErjPbJBp5VApH+iShP1mqkwohyZ4LbXnRQS8ViDAShEZbIz5T5z7r1G2tdQKRKTQ95MPNMY7b7A1dMLzljCvu1REeopIA+B24AVjTDnwD2CgiJwpInmhffYLc2I1GpcDT8Xx+myQseUgIpeEzveIiHQE/ox3OiCIMrUctuGdr+sV+jk7lP4dYHb0bzM62VapvYFXYJU/t+KdBK6P903zX7zuxsEm4lU0a4FC4JfgjZQBg4Cb8U7srwBuIsznEvpH2VnLCeqTgHYE/1KOTC6HnsAHwE68yzsWAsloAWaCjCwH41lb+RPaF8A6Y0xprG82UhI6qaeUUoGQbS01pZSqkVZqSqlA0UpNKRUocVVqInKWeJOHl4jIqERlSimlYhXzQEHoYr5FwOnASmAOcLEx5qvEZU8ppaITz8W3xwNLKi8yFZFJeMPB1VZqdaWeKaRhHIfMLXvZRanZJ7VvGR0th+gkoxy0DKK3gy0bjTFV5qEeLJ5KrYgDp1usBE44eCMRKQFKAAppwAnSP45D5pbZJnHXjGo5xC5R5aBlEJ93zAvLI9kunnNq4b65qvRljTHjjDF9jDF9CqgXx+FUPLQc0k/LIDXiqdRWcuAcsnbA6viyo5RS8YmnUpsDdBORTiJSFxgCTE1MtpRSKjYxn1MzxpSJyHV46y3lAU8YY+YlLGdKKRWDuJYeMsa8gTepVimlMoLOKFBKBYpWakqpQNFKTSkVKFqpKaUCRSs1pVSgaKWmlAqUINxNSmW5vJ7dAVhwTXObtvhHD9u4wjf7rk5odt5DWzvZtAn3nG3jluM/TFo+VXbQlppSKlC0paZSJr+9u9PaV7ccZuPnfvAoAL3rVdi0Ct/3bQUuvfJ7uKTZEpvSduQzNn5i2vdsXLZyVfyZDoA6hYU27jDLrUPxUNH7Ns4T73OdX7rbpt1w5uU2Ll/oPu9Mpy01pVSgaKWmlAqUnOh+rvn1d20svhXfCjd5D7Yc7tLafFjunn/1o6TnLei+ufMkGy+45EEbhzv57+9yvr67qY0/2tm5yn6/03CZjS9otN3Gq6d9aePXjmxOLqvsdq6a5AZVXit6Juy2/b48HwC5u5VNq/f1pzEdN7/Y3d+4bNm3Me0jHtpSU0oFilZqSqlAyaju5/rhrpu49Zj9Nn75jLFx7feIunPCpu81ZQA0rVPf5eGyXTZefb/7eO5ZezoAmy5sYtPKVqyMK1+5YPDpboTN3+UMN6L54NYuNuXtM4+0cbhRzPcHDrHxeY+4a9r8o6Kv0Te2TAfEklt7A7Cg74Nhn+82/Uob97hmIQAVu5bZtGjuM7donPusp5zxgI0veurXAHS49YMo9hYfbakppQJFKzWlVKCkvfu56DHXbF1w9n02ricFvq2Sc+edA4/hOTSvoS926U93nAXApc/3s2lbfpLeUZ6MdvzRAFzd0nUNX9/tLrj1j2h+ub0tAPtucrd0/PpO9+F3/1MDG5fPXwwcODJd8Kjbdr+vz7RqpHc6o+hvqev6pJs56Vgbz/rJ30OR+/y+LXMX13Yf5kaKK/aXRn2s/QO+Y+OXT3eniI4sqBv1vhKp1paaiDwhIutF5EtfWgsReVtEFod+5/bYuVIqY0TSUnsKGAs87UsbBUw3xowWkVGhxyNjycDD33e79bec/rapm43XlzaOeH8vzfW+PTq8GtsNtVf2d/X8nWc/a+PKa6H+UTzTpl36bD8bb7nITQHSAQTgoy8AKLngGpuUt2azjQ88+b8WgFUjXett/mnuZPMPH7vK7WO+93vTMHf9234z18b+AYiOz3j3vi2LJf9Zat1I1+I6NM9roe0xLu3yETfYuMH+2XEda+ev3PWBR9d1/7s7zT4bd/rnJgDc1Z/JV2tLzRgzC9h8UPIgYEIongCcn+B8KaVUTGIdKGhtjFkDEPp9aOKypJRSsUv6QIGIlAAlAIW+E5aVxlz0Yxv/vpe7BuzQVxbauHzTwQ3F6nUn/DVpker6qosff8Kt07V2kne91fBmK2yavyvao8R1s4r/kHndz9rKIVnMnC9sXFs3sHCjO8s/bluxjeuu22njb27zTv4/dZnrnlZOswKYu899T2faKh2pKIOS7u9VSfu/hYNt3ODl8F1OyfeqAqlfP+zzfuVHe6cJ7j3iybDP95v7UxsfOm9BrftLtFhbautEpA1A6Pf66jY0xowzxvQxxvQpSNIopqqdlkP6aRmkRqyV2lRgaCgeCkxJTHaUUio+tXY/ReQ5oB/QSkRWArcAo4HJIjIM+BYYXP0eambmzrNxSzeIldLRkupUfO6azk/eey4Aw297OOy2z17qrrG7+Q/HJzdjWWrPIPe5bD7c/elVdjtbfuG6mSVNl9m412vLbXx8PW9b/yjnHF+X8/fDfCOl/C8Buc5+jQv22niXL33/GX1s3OIPywB4vvO/Itjju1VS3veVwSGj09sKrbVSM8ZcXM1T/ROcF6WUiptOk1JKBUrap0mp3LH6IncR6PzTXDfeLRJZdeFIcF1Of7p/lPOyF66zcecZuX03qXGPDbTx1Td6U5ee7uyG9K/+4Cwbj+/oyiAf35zAGFzx6tU27vbhf+PaV7y0paaUChRtqdVg5c1ufbeK3jtq3LZ1nmuFlP3Am6qV/++51W2e88KtpxYu7eD0khU/AGDFb900ulxvnfntaldRJa2+uAnmEzr+2/eMa53dsNYbxHljmltgYn8b9ze95IzHajxuq//FNi0xGbSlppQKFK3UlFKBkhPdz/zOxTZeMqyNjR8aMq7G1/UrdNc5Vd7stTrt8hvZeNyT3jVr13Y8JZpsBl7b5103aHCRO6F9VJPVAFzd0q17VpTnn0bkPvuv/3oEAPVn6J2+wun+6AYbH7F/eI3bdp3oph9WLPwagE5lriv/zeiTqrzG79pVJ9u4xbPuVEs0y4Ang7bUlFKBopWaUipQAtf93Dn4BAA2HOfq69t/NMnGQxpviWJvsdX5A94ZAUB3Po7p9UFVf4rrMu7zzRaeG/qcS/q6lU52/MlN6Pn30c/b+JRbvWugPpvb3qZl2moc6VS+6Gsbdxr1dQ1b1j4VMX93zSOaHz/ey8at9mfOCLS21JRSgaKVmlIqULK2+ym93c1um41dY+M3ir2pH7WNVgK8sssbsfxyT7uwz792Zz8b5+3zxnSG3u6mnJQ0XR32dXXXVr1LVZDkt0/O/Rj8C0o2crN5GPyuGyl9uesbABx1pRtZ7nCrdj+TQarpn5aFOq7NF+0Lv0GaaUtNKRUoWdVSW36bm7b0hyHu5PEljTfZuPK+hgtK3V37fvHclTZusMad/GwzcyMA5V8tCnu8plSdmLv4t619G7iW2tL9bi2w4ik7CaLK9dAqT9YDvLbctZjbnD8/Kcfddpe7v2rFI16LeX+3PUk5lnJ+evG0sOmDl3gt57yZmblenbbUlFKBopWaUipQsqr72ayvu7+Lv8vZ/6vzbLz/gcOAA6+JKib8NTTRLBlecVpvAM5vNt6X6r4TNle4KUCVN/INAv+gwEV/fROAj7cX27RkdTnzmjW18Y9Hu26Qf501lXh5hxxi4271loTdZuPDxQA0Dt2EOtPU2lITkfYiMkNE5ovIPBG5PpTeQkTeFpHFod/Na9uXUkolWyTdzzLgBmPMEcCJwHAR6QmMAqYbY7oB00OPlVIqrSK58coaoPJu7DtEZD5QBAzCu8sUwARgJjAyKbkMaTnMjSp2/bWbUtPlJte9zOfbpBx7S/dCAE4uDP89UPLlpTZuRfjR1Gy0/Cdu5LGkqTe36d5PBti0LnySuIMdf7QNf/jkLHfcZq4bVBH6Hi5YVPtNd1X0tn2/i40HNnDd/p3GXZNWuHF/SvMUragGCkSkGOgNzAZahyq8yorv0ERnTimlohXxQIGINAJeBEYYY7aLRHbCVkRKgBKAQhrUsrVKFi2H9NMySI2IKjURKcCr0J4xxrwUSl4nIm2MMWtEpA2wPtxrjTHjgHEATaRFXOvHla1xoy1dbkrtyMumvmVV0uaX7rZx44eaVnk+k8RaDkUz3L0ZCq731rS/vpdb5378L86xcct5rosS7v4MeT2723h1/1Y2bnSOV5Yzjn7KpvlHOSt8HYrub/7c+32bW1AyWyTyfyFZht42NWz60v2uDAreyex7b0Qy+inAeGC+MeYe31NTgaGheCgw5eDXKqVUqkXSUjsZuAz4QkQ+DaXdDIwGJovIMOBbYHByspg+Z3653cYvN3swFLnr0YbOG2rj5m/OSVW2Ust3zd3Jn/8IOHB9s6tHPWBj/12fblv/nSq7Oq/pczbuXc9tW6eWu0n1eMEtS93z7ysAb0heJV7LvPBT/O5ac6bv0dbUZCZGkYx+vgfVXvHYP7HZUUqp+Og0KaVUoGTVNKlU+3GTz23coI639tqi/W6Z6QZjm6U8T+nU7Crv5ra3TXVdy7+0dp/Rft+p7z8d6p2pqPDdW6i6k//ryr0VNx7a5FZh+ddYd6eibuPddYja7UyP0oq82jfKENpSU0oFilZqSqlA0e7nQdZf67pArfPciGblIpAX/+Umm9bqzcy5g04qVC7d/dlAdyenrn+rOsoJML/f4wCc+vmFNm3D5iZht+06xutU+pfzblnNyioqPR4rfs3G37n7VwB0uaHqIqqZQFtqSqlA0UpNKRUo2v0EpF49G19wtZsCtKOi1MZnf+StCtLhUe0W+W8e3OWS8HdyOhevW9oEd0Pd8J1PyMj5Qjnqd5MusfHhl7sJRIcXuP8RKjJ7oU5tqSmlAkVbagAVrq0w8dXv2/jNz/rZuMPkzDwpqlQidfyj64n8+o8nhd2mS4YP4mhLTSkVKFqpKaUCRbufgNnvBgSKf5fZTWulVM20paaUChSt1JRSgSLGpO4qIRHZAOwCNqbsoKnVisS+t47GmENq3yw6AS+HRJcBJKEcQmWwnOTkNxOkrRxSWqkBiMjHxpg+KT1oimTTe8umvEYj295XtuU3Uul8X9r9VEoFilZqSqlASUelNi4Nx0yVbHpv2ZTXaGTb+8q2/EYqbe8r5efUlFIqmQLf/RSRmSJyZapfqw6k5ZB+uVIGWVOpicgyERmQ7nzUREQ6i8hrIrJDRDaKyJ3pzlOiZXo5iMhRIjIt9PkHshuSBWUgInKHiKwSkW2hCvHIVB0/ayq1TCcidYG3gX8DhwHtgH+kNVO5aT8wGRiW7ozksMHAz4DvAS2AD4GJqTp41ldqItI81DraICJbQnG7gzbrIiIfhb41pohIC9/rTxSRD0Rkq4h8JiL9YszKFcBqY8w9xphdxpi9xpjPa3tRUGRKORhjFhpjxgPz4ng7WSlTygDoBLxnjPnGGFOO9+XeM8Z9RS3rKzW89/Ak0BHoAOwBxh60zeV43xxt8W4deT+AiBQBrwN34H2j3Ai8KCJVrloWkQ6hwu5QTT5OBJaJyJuhrs9METk67neXPTKlHHJZppTBJKCriHQXkQJgKPBWnO8tcsaYrPgBlgEDItiuF7DF93gmMNr3uCdQCuQBI4GJB71+GjDU99orI8zfv/C6Pj8E6gI3Ad8AddP92eVSOfhe39X7807/Z5ZrZRD6+78Pb6X2MmAp0ClVn0/Wt9REpIGIPCoiy0VkOzALaCYi/ltKr/DFy4ECvLlpHYHBoW+drSKyFTgFaBNDVvbgNbnfNMaUAncBLYEjYthX1smgcshZGVQGtwB9gfZAIXAb8G8RaRDDvqKW9ZUacAPQAzjBGNMEODWU7r87RHtf3AGvRbURr4AnGmOa+X4aGmNGx5CPz8nte4hkSjnkskwpg2OB540xK40xZcaYp4DmpOi8WrZVagUiUuj7yQca47WStoZOet4S5nWXikjP0DfF7cALxp3AHCgiZ4pIXmif/cKcXI3EP4ATRWRA6JtxBN4fy/xY3miGy9hyEE8hXheI0L7q1fKybJSxZQDMwWv1tRaROiJyGV6LcElM7zRK2VapvYFXaJU/twJjgPp4Fch/CX9CciLwFLAWrzn8SwBjzApgEHAzsAHv2+omwnwuoZOjO6s7OWqMWQhcCjwCbAnt97xQVzRoMrYc8LpRe3Cjn3uAhVG+v2yQyWXwN+Az4FNgK/Ar4AJjzNbo32b0dJqUUipQsq2lppRSNdJKTSkVKHFVaiJylogsFJElIjIqUZlSSqlYxXxOLTTCtwg4HViJN+JxsTHmq8RlTymlohNPS+14YInx5neV4k2NGJSYbCmlVGziuZlxEQdenbwSOKGmF9SVeqaQhnEcMrfsZRelZp/UvmV0tByik4xy0DKI3g62bDQR3E0qnkotXCFX6cuKSAlQAlBIA06Q/nEcMrfMNtMTti8th9glqhy0DOLzjnlheSTbxdP9XMmBUy7aAasP3sgYM84Y08cY06eAIF7YnR20HNJPyyA14qnU5gDdRKSTeAskDgGmJiZbSikVm5i7n8aYMhG5Dm95kjzgCWNMzi3Mp5TKLPGcU8MY8wbeHDSllMoIOqNAKRUocbXUAqmOW09v0WO9bTzvzIcAGDj0GpuWP31u6vKllIqIttSUUoGilZpSKlC0+wnkd3SX2y36a0sbL+33uG+rugBs7VLXprRK3LWxClg66Rgbv3fywzb+yeW/sHHejP+lNE8q+2hLTSkVKFqpKaUCJae7n/mdiwH46netbNqBXU7nqhUnA9D6PxttWnnyspaTzLdugnfL79W38eYebkrRITNSmqWctu+cvjbefNVOG3/S95kaX3f1yu/Z+L03jwWg86Pf2LSyNWsTlcWwtKWmlAqUnGupSYE70T//1hYALB0QvnXW+Z2f2bhHibf2ZcXexUnMXW5ruDL86j6HXeQWZyh/JFW5yS2V/xeL7nHXZr4+8F4bdy1wreWKWvb1SLv/uG2vmgVAr6Mvt2ntLtCWmlJKRUwrNaVUoORc93Ph2GNtvHTAY1We7zrzCht3u9xdE1Vbk1slz56yAhvXrWE7FbuFD/QCYNHAh2xaHQptXFF1/dcDlKzoZ+PH279b5fn7e02y8d0tT7Nx+abNUee1NtpSU0oFilZqSqlAyYnu55IxJ7r43Id8z3h1eue33Shn9xK3zmVsNw9UsWpyzpqw6dtebGvjQ4homXpVDf/of2WXE2DeuWNDkVulZk35bhuf+vKNNu78cikA9Ra7UczyjZts3Pv5S2w8t+8/APjfnmKbZkr3x5j7yGhLTSkVKFqpKaUCJbDdz9Kz3BSPl88fY+M8cSM6lSOd3X76mU0zFTr5KdXK+x0HwKtHPmjTPi113aDWz3xpYx2Fjs+a4X1svGjgA75nvM97/LYONuWlq063cbf3/1tlX2XVHGPfvoIqaa+uciuw1N+xNMLcxqbWlpqIPCEi60XkS19aCxF5W0QWh343T2oulVIqQpG01J4CxgJP+9JGAdONMaNFZFTo8cjEZy92LX/nvg2OqetaZ6fPH2jj7rdsB6BcW2dpVV7P+25tJG4qzn7jhmkqduxIeZ6C6pqSKTau47sf+V839QTgw/O62zRZ9mmN+8pr0sTGK688ysa/OeYlG39S6rWt65+Z3NaZX60tNWPMLODgK+QGARNC8QTg/ATnSymlYhLrObXWxpg1AMaYNSJyaHUbikgJUAJQSIMYD6fipeWQfloGqZH0gQJjzDhgHEATaZGyS79+WfRO2PTtE9rZuNniD1OVnbRLVzlEYtn/5cYgfCaUQbmvc+af+vTGX/oB0HhZ1QEB4IC7rJWf5k01PHesW8/+6mZuoTt/t/achZWduFWxZjlqsf41rRORNgCh3+sTlyWllIpdrJXaVGBoKB4KTKlhW6WUSplau58i8hzQD2glIiuBW4DRwGQRGQZ8CwxOZiajse1Sb0rUqYVu5Obkz39k42YTq2leq7RpfJiObqZbg7WlNT5f2eUEePMfVVe38fu/JWfbuM4F3lSrVF5fUGulZoy5uJqn+ic4L0opFbfcOEOrlMoZgZsmtfW8XVXSdk89zMaNzDdVno+Zb0QIvYBXZYHFe1q7B02X2fCJp+8HYPS6ATZt5vKuNn7r+Pt9e/Hu9LWtYq9N6fv6r2x8+A1upZuKXVX/H5NNW2pKqUAJXEutTfPtVdLqb4p/GvS+H3oT5Dde5daYOqq1W/9rx4/dOlXJvq9hENQpdFPXTimqOoXmsfWn+R7trPK8is384T3dgxdn27BNntf6uq/t+zatTtsPbFyBuw9rpe8/cJONu9/p3za9tKWmlAoUrdSUUoESiO5n/mHu5OdjPZ4JRY1i2ldes6Y2Pv9Dd+Piixp7J0qb1qnaDAc4cqxbwjjZN2sNgjq+z/mBtm9Wef7d99yqD13Qawvjse8ct7bgiiFuFTT/dKZw8sTX5jGuU9l/nnfdZ1tflzOTaEtNKRUoWqkppQIlEN1PCtzywR3yo+92rr/2uzY+/+czbVzSdLVvq/DdzkqHNE799TjZrKy4dY3Pd3gruXccCqo6xxxu48PGeStjPN7+UZvmX5kj3CjlqLWuq/rSR27p74dPn2Dj8T28O0RdfqG7w1SjyZlzikBbakqpQNFKTSkVKIHofhrfGvbjtnk3vj2w6+jktWpp4xU/6wHAFyMeCrttNLbtcReTVrsMsLI2/m5vlbSzF5xn47ozfXf4SkWGstjGkpNsPO0Pd9m4aZ3Kv8nwo5w3rHE3+X7z315Xs/u97kLo7ms+svFd33ej+5WrdAy5xY1avzY5c+69pC01pVSgBKKlVr51m42fW+md6Cxp6tatPHmkmw7S909uQvuFjdxyxLG4bYObctL2l26goLr7ISrn4aOe8T3yFgZYvd3dnaht2coU5yi77BjiWlnhW2cwf7832HLvWnf/zoVjjnTbvuLWHOy811vavrq/3bx3Xcv58MnDAfhssLuf7stnXGfjgn99HNF7SBZtqSmlAkUrNaVUoASi++m398k2AOz7u7vO6e+HfRL3fvcbb720nu8Os2ndf7vJxmXLV8R9jKDLL+5g48biptjkSUG4zVUNNh7jTv77u5wv72ph4ycvPAeAik+/smmNfVPOollNo059d4wjj1sGQD1fuVXk1zzlKpVqbamJSHsRmSEi80VknohcH0pvISJvi8ji0O/MGf5QSuWsSLqfZcANxpgjgBOB4SLSExgFTDfGdAOmhx4rpVRaRXLjlTVA5d3Yd4jIfKAIGIR3lymACcBMYGRSchmFJs96zevZd7im8amF1W1dVblvNYI+H//ExnVf8BqinSe6GyDrKGd09j7u4u4FrlAqP/NGk5sc/BIVAf9qGyNnXGjj7p/OiWu//ms6G7zsjvF85zdCUeZ0Of2iGigQkWKgNzAbaB2q8CorPr3mVCmVdhEPFIhII+BFYIQxZrtIZLW0iJQAJQCFNIgljyoBtBzST8sgNSKq1ESkAK9Ce8YY81IoeZ2ItDHGrBGRNsD6cK81xowDxgE0kRZpn/Fy+HuX2Vi+bAxAp/vd3W9Muet+HrpjQeoylmTpKoe87l1sfEPx1LDbXLzUuzi0yaTZYZ8PikSWQavP3cu3VOyx8Zyz3QWxfR8dAcARf1xu08rXhf03Jb/Im16469gimzbivudsfE4Dd4F75X/Ig1td2db/z4Iqz6dLJKOfAowH5htj7vE9NRUYGoqHAlMOfq1SSqVaJC21k4HLgC9EpHJexc3AaGCyiAwDvgUGJyeL8ev58LU2Lv6rm6RryrxT/XrHzuQpLXLLdvevvy/sNoue9xYWaG0yc3noTNR4krve7NSu7q5On13zgI0XnfsIAPPOcENaIxZfFHZ/zxzhTVvzX/PmH4Dwt74qJ8Iv+IWbJig7PiNTRDL6+R7VD3P0T2x2lFIqPjpNSikVKIGbJlXpz5172bg9rluT9pEKBcDVK79n47bPLQT0NECsWixwn9wjWzvbuGeht9JJv0LX0Xr7yBer2UvVizkf2dbRxve+fq6Nu/3Bm3YoezOny+mnLTWlVKBopaaUCpTAdj9VZsib8T8bn110nO+ZXdXEKloNX3DX9732gltX4q3iYwG4ZnSzsK/763Gv2PiDHV0BeHXaCTat081uSmAXXJzu69Bqoy01pVSgaKWmlAoU7X4cLigDAAAR6ElEQVQqFVBly74FoNOQb8M+P47Ovkdep7KTr5uZrbSlppQKFK3UlFKBopWaUipQtFJTSgWKVmpKqUDRSk0pFShaqSmlAkWMSd26FSKyAW9OzMaUHTS1WpHY99bRGHNIAvcHBL4cEl0GkIRyCJXBcpKT30yQtnJIaaUGICIfG2P6pPSgKZJN7y2b8hqNbHtf2ZbfSKXzfWn3UykVKFqpKaUCJR2V2rg0HDNVsum9ZVNeo5Ft7yvb8huptL2vlJ9TU0qpZAp891NEZorIlal+rTqQlkNmyIVyyJpKTUSWiciAdOejOiJylIhME5GNIhLY5m+mlwOAiPxKRNaKyDYReUJE6qU7T4mW6eUgIvVE5F4RWS0iW0TkIREpSMWxs6ZSywL7gcnAsHRnJJeJyJnAKLx70hYDnYHb0pmnHDUK6AMcBXQHjgN+n4oDZ32lJiLNReQ1EdkQ+kZ4TUTaHbRZFxH5KPTNPUVEWvhef6KIfCAiW0XkMxHpF0s+jDELjTHjgXlxvJ2slSnlAAwFxhtj5hljtgB/Aq6IcV9ZJ4PKYSBwvzFmszFmA3A/8LMY9xWVrK/U8N7Dk0BHoAOwBxh70DaX432gbYEyvA8YESkCXgfuAFoANwIvikiVq5ZFpEOooDsk6X1ku0wphyMB/w0pPwNai0jLGN9XtsmUcpDQj/9xOxFpGuP7ipwxJit+gGXAgAi26wVs8T2eCYz2Pe4JlAJ5wEhg4kGvnwYM9b32yijz2dX7WNP/meViOQBfA2f5Hhfg3cO6ON2fXY6Vwx3A+8AhwGHA7FA5tEn2Z5P19ygQkQbAvcBZQOX9wRqLSJ4xpvLW1St8L1mO94feCu/bbLCIDPQ9XwDMSG6ugyeDymEn0MT3uDLeEcO+sk4GlcOfgWbAp8A+4DGgN7A+hn1FJQjdzxuAHsAJxpgmwKmhdH/Tt70v7oB3Un8jXuFONMY08/00NMaMTkXGAyZTymEecKzv8bHAOmPMphj2lY0yohyMMXuMMdcZY4qMMZ2BTcBcX8WaNNlWqRWISKHvJx9ojHfeYGvohOctYV53qYj0DH2L3Q68EPpw/wEMFJEzRSQvtM9+YU6s1ko8hUDd0OPCIF5KEJKx5QA8DQwLHac53ojbU7G8ySyQseUgIkUi0jb0f3Ei8Idq8pJw2VapvYFXYJU/twJjgPp43zT/Bd4K87qJeH/Ya4FC4JcAxpgVwCDgZmAD3jfVTYT5XEInRnfWcGK0YyhPlaOfe4CFUb6/bJGx5WCMeQu4E6/LtDz0k5J/pjTI2HIAugAf4C1xNQEYZYz5VwzvMWo6TUopFSjZ1lJTSqkaaaWmlAqUuCo1ETlLRBaKyBIRGZWoTCmlVKxiPqcmInnAIuB0YCUwB7jYGPNV4rKnlFLRiaeldjywxBjzjTGmFJiEN3KilFJpE8+MgiIOvDJ5JXBCTS+oK/VMIQ3jOGRu2csuSs0+qX3L6Gg5RCcZ5aBlEL0dbNloIribVDyVWrhCrtKXFZESoASgkAacIP3jOGRumW2mJ2xfWg6xS1Q5aBnE5x3zwvJItoun+7mSA6dbtANWH7yRMWacMaaPMaZPAUG9wD7zaTmkn5ZBasRTqc0BuolIJxGpCwwBpiYmW0opFZuYu5/GmDIRuQ5vaZI84AljTE4ukKiUyhxxLT1kjHkDb/6ZUkplBJ1RoJQKFK3UlFKBopWaUipQtFJTSgVK1t+jING+vvtEG19/1ps2fuPikwCo+HxByvMUeCceA8DS69313ItOm2DjrjOvsHGXn3yasmyp7KQtNaVUoGilppQKFO1+AvlFbW08dtCTNj69/h4bTzjhbABafp66fAXZ2hHftfFfrnsCgDPq77Jp+32ziO87fpKN7+fwKvta9wu3r7bPutMD5Zs2JySvKrtoS00pFSjaUgO+/nlHG/tbZyp+Us9N3N5y4XE2nnXj3TZuIHWj3u/K37rW2ZzhY2w8ebi7m9v9Yy4A4JBHPox6/7mgzrFH2Hjhr+sDcFmv2TbtFy0+snH/u2+y8WFjPkhB7mKnLTWlVKBopaaUChTtfgLtT16Z7iwE1je3ui7nvMvH+p6pucv5yNbONn504jk2LsLr+uxrWWHTCiTPxpc0XmPjvqPuAeAyfm3TcrEr6j8FsLbkOzaePeo+G++oKAXgxEk32rRZvbra+LRL59h4oevtZyRtqSmlAkUrNaVUoOR093PvuccDcF/nB3ypBenJTMBUdnka9twS8Wve3N3Yxi/+5gwbF70e22hb9wKvizvpt3fZtDN7j3DP/3xOldcESZ3CQgAWjDnGpi0Z6E4BPLC1m43/edtZAHSZ7Lrned272PjzLr1sbAZ609nyd5fbtPzpcxOV7bhpS00pFShaqSmlAqXW7qeIPAGcC6w3xhwVSmsBPA8UA8uAC40xkfczMsSelt6o2dF1tcuZCJLv/py+vt0b9fyqz9jqNrdKVvQDYP0FrvtZb1XNXcPi10ttfEzHK2w896TxNq4cFe2UX2jTmiwIdlnXadDAxque9S4qX9L3EZt2zxbX5Zz2i9Ns3GjGf6vsq3zR1zZusGW7jUd8OBOAx9eeatO2Je5ujnGLpKX2FHDWQWmjgOnGmG7A9NBjpZRKu1pbasaYWSJSfFDyIKBfKJ4AzARGJjBfGeH9fa7Ob7yiLI05yQ77BvS28VeX1txCu371yTZed47XeirfVOW2sdXKm/E/G3eY4dJfXtjGxhc2Wh/x/rKZv3W24O6jbFzZQrtrcw+bNuu8njbOW+o+w9qsuMK18PrXnwbA5kPc659u5gYjyrdui3i/yRDrObXWxpg1AKHfhyYuS0opFbukX9IhIiVACUAhDWrZWiWLlkP6aRmkRqyV2joRaWOMWSMibYBq2/nGmHHAOIAm0sJUt106HH51zfdeHrPydBvXfSu7r2lKVjms+6VbLePaa16pcVt/l3Ppaa6TULE7N9Y9S1YZbLjkWBsvOe9BG7++uxEAswYdadPKli6L6RilTatmd/5etw5hurucfrF2P6cCQ0PxUGBKYrKjlFLxqbVSE5HngA+BHiKyUkSGAaOB00VkMXB66LFSSqVdJKOfF1fzVP8E5yXlrm1dOWwmYZ9f+KYb8WnHhhTkKDv4Fxcc/Ut3XVj/+rurbFt5DRq4UU5IbJdTervuVXFB1RG9Jfv32bjpN8EYxfYvQf+bm5618apyVwZ/veVaAJp8U/UatIiO0bnYxuf+cHb1G2YYnVGglAoUrdSUUoGS06t01KbjK67LWV7DdrnmexNdFy9cl9NvzitH27hoU3LWtl94jbs84vh6VUfppu1yF5zWn/JRleezUUXLJja+oKGboXj7xhNs3OTZyLudlVPcVo043qaNuup5Gw9plD2nX7SlppQKlJxrqfmvq+pRUNlycBOe/SdaKdP2md/Gn58EwDXN7/aluqWi15S7O3H9evn5AHR4aZ1NS+Snmd/J3QHs3bPu9T1Tv8q2723u6nu0MYG5yDznNfnExq+VXA9Awe7wl8RtPseV12vffQiALvmuJfvKrmY27jr1ahsvOc+bfjVnsysDiHyKW7JpS00pFShaqSmlAiUnup95rd18+94/+cLGTeoUVtm238vubjrdFsd2fU9Q7Qj1NhrVqRf2+bvWf99t+73Kbl5yunsLh7vVONrkVe1yAmyp2AvA2vvcstQNA9L9rPhioY27T77WxosufMjGH93yIJF6a09LAM5//Gc2rcOdbonuw3u49dQ4z/u1eI7rfnbW7qdSSiWHVmpKqUDJie4nrZrb8PH2b1V5enuomwLQeKnW87F6650+Nu5Ekm4aLN6UNpNXy3bAjSt/CEDDF7Jnik/EjBvR7Pord5rk+AXDbVxxdtUV9reud0umF7/o0itXoWmPu5bQP2ZqPl9g4zs2egtRXnrmuzbtg9/UfHPqVNL/YKVUoGilppQKlJzofpY3rLlp/MV+N83msDHJmcqTC9q8n/yLlbdd4k0DWnBh7SN7H7zvTY/qQu6MYrd61Nftf7Tq87Guu5/XsoWNezfwPs+5uzvFuLfk0paaUipQcqKl1vjuNTU+f80nl9i4HTUv8a2q1/FmdzJ53avx7Su/XZGNFw/vYOPZl1ZO0Qp/rdxzO1rbuPuT3olynewWP1Pk2njnNNgJwPX/OcmmdefjlOepOtpSU0oFilZqSqlACWz3M799Oxt3b/Rt2G0uWTYAgI5Xuike2lWJ3SnNltj4lW4nAlC++JtaX5d3hLds+uKhrWzamB8/aeMz6u/ybR2+21lpwvBBNs6fN7eGLVU0Vp3eokpa/saCMFumXyQ3XmkvIjNEZL6IzBOR60PpLUTkbRFZHPrdvLZ9KaVUskXS/SwDbjDGHAGcCAwXkZ7AKGC6MaYbMD30WCml0iqSu0mtAdaE4h0iMh8oAgYB/UKbTQBmAiOTkssYrD27vY2nHjrVxnni6vEte73r0+qUuukkUuCuaTP7S5OZxazT7XFvFPm2s3vZtFsO+dTGP22ywsZ5UysA+GK3Ow1QnV4Nvek2lzSueZTab+ou1zG48Z0hNj78v270uiLivana7GueUfchr1FU59REpBjoDcwGWocqPEJ3ag97XZ+IlAAlAIU0CLeJSgEth/TTMkiNiEc/RaQR8CIwwhizvbbtKxljxhlj+hhj+hTUcpJXJY+WQ/ppGaRGRC01ESnAq9CeMca8FEpeJyJtQq20NsD6ZGUykcqN65S8cXioW7rIPd/tBbfgXrfrc2d6TSTKvlkGwLT7T7FpI25zn1FT36KblzdZ5QWVv+Ow27jTAA9u9rq+s37W16Z1/9itq69dThXJ6KcA44H5xph7fE9NBYaG4qHAlMRnTymlohNJS+1k4DLgCxGpPCt8MzAamCwiw4BvgcHJyWJsCje77+yvy9xdc7rkV136eY+vJdBgjV6PXJsWT7hJ03+8pr+Nrz5kpo2PKIjvGqYHt7oluCfe90MbtxpXeewv49q/il3lYFvzDJ1RGMno53uAVPN0/2rSlVIqLbRZopQKlMBOk2r0T7eE84WH3WTjT3/r7rZzx8bDAXhx3A9sWtFYXU8tGl/3dUuhj+p6sUu/4jAAzjzLrd5wdxs3qHDk09fZWMLMTevy7CYbt/oqSUuDq5hUDrY1n78zzTkJT1tqSqlA0UpNKRUoge1++rV+wHUpz3ygV5XnD0W7nIlQvmSpjYt/78ULf++eP5fv2Li2u03paimZyz/VMBNldu6UUipKWqkppQIlJ7qfSqnE+Xq/N+qZt3W3Tcuk0wXaUlNKBYq21JRStSr+vRvYufb3lQsafJ2ezNRCW2pKqUDRSk0pFShaqSmlAkUrNaVUoGilppQKFDEmdXeJEZENwC5gY8oOmlqtSOx762iMOSSB+wMCXw6JLgNIQjmEymA5yclvJkhbOaS0UgMQkY+NMX1SetAUyab3lk15jUa2va9sy2+k0vm+tPuplAoUrdSUUoGSjkptXBqOmSrZ9N6yKa/RyLb3lW35jVTa3lfKz6kppVQyafdTKRUoKa3UROQsEVkoIktEZFQqj51IItJeRGaIyHwRmSci14fSW4jI2yKyOPS7ebrzerCglAFoOWSKjCsHY0xKfoA8vGn9nYG6wGdAz1QdP8HvpQ1wXChuDCwCegJ3AqNC6aOAv6U7r0EtAy2HzPnJtHJIZUvteGCJMeYbY0wpMAkYlMLjJ4wxZo0x5n+heAcwHyjCez8TQptNAM5PTw6rFZgyAC2HTJFp5ZDKSq0IWOF7vDKUltVEpBjoDcwGWhtj1oBX0MCh6ctZWIEsA9ByyBSZUA6prNQkTFpWD72KSCPgRWCEMWZ7uvMTgcCVAWg5ZIpMKYdUVmorgfa+x+2A1Sk8fkKJSAFeAT5jjHkplLxORNqEnm8DrE9X/qoRqDIALYdMkUnlkMpKbQ7QTUQ6iUhdYAgwNYXHTxgREWA8MN8Yc4/vqanA0FA8FJiS6rzVIjBlAFoOmSLTyiHVq3ScDYzBG/15whjz55QdPIFE5BTgP8AXQEUo+Wa88wiTgQ7At8BgY8zmtGSyGkEpA9ByyBSZVg46o0ApFSg6o0ApFShaqSmlAkUrNaVUoGilppQKFK3UlFKBopWaUipQtFJTSgWKVmpKqUD5f1fT8jGEHvVnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Review a few images\n",
    "image_list = mnist.train.images[0:9]\n",
    "image_list_labels = mnist.train.labels[0:9]\n",
    "\n",
    "# https://matplotlib.org/mpl_toolkits/axes_grid/users/overview.html#imagegrid\n",
    "fig = plt.figure(1, (5., 5.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(3, 3),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.3,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for i in range(len(image_list)):\n",
    "    image = image_list[i].reshape(28,28)\n",
    "    grid[i].imshow(image)\n",
    "    grid[i].set_title('Label: {0}'.format(image_list_labels[i].argmax()))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark CNN (for testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on challenge requirements, building model using tensorflow low-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "# To run nicely in jupyter notebook\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for creating weights and biases\n",
    "# https://www.tensorflow.org/get_started/mnist/pros\n",
    "def weight_variable(shape, name=None): \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name=None):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "# Functions for convolution and pooling functions\n",
    "def conv2d(x, W, name=None):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME', name=name)\n",
    "\n",
    "def max_pooling_2x2(x, name=None):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create placeholders nodes for images and label inputs\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='xOriginal')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10], name='yOriginal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark model to be used for testing classification on real images with and without noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y = (Wx +b)\n",
    "# https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "# Input layer\n",
    "x_image = tf.reshape(x, [-1,28,28,1]) # mnist image comes in as 784 vector\n",
    "\n",
    "# Conv layer 1 - 32x5x5\n",
    "W_conv1 = weight_variable([5, 5, 1, 32], name=\"W_conv1\")\n",
    "b_conv1 = bias_variable([32], name=\"b_conv1\")\n",
    "x_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1, name=\"x_conv1\")\n",
    "x_pool1 = max_pooling_2x2(x_conv1, name=\"x_pool1\")\n",
    "\n",
    "# Conv layer 2 - 64x5x5\n",
    "W_conv2 = weight_variable([5, 5, 32, 64], name=\"W_conv2\")\n",
    "b_conv2 = bias_variable([64], name=\"b_conv2\")\n",
    "x_conv2 = tf.nn.relu(conv2d(x_pool1, W_conv2) + b_conv2, name=\"x_conv2\")\n",
    "x_pool2 = max_pooling_2x2(x_conv2, name=\"x_pool2\")\n",
    "\n",
    "# Flatten - keras 'flatten'\n",
    "x_flat = tf.reshape(x_pool2, [-1, 7*7*64], name=\"x_flat\")\n",
    "\n",
    "# Dense fully connected layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024], name=\"W_fc1\") # max pooling reduced image to 7x7\n",
    "b_fc1 = bias_variable([1024], name=\"b_fc1\")\n",
    "x_fc1 = tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1, name=\"x_fc1\")\n",
    "\n",
    "# Regularization with dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x_fc1_drop = tf.nn.dropout(x_fc1, keep_prob)\n",
    "\n",
    "# Classification layer\n",
    "W_fc2 = weight_variable([1024, 10], name=\"W_fc2\")\n",
    "b_fc2 = bias_variable([10], name=\"b_fc2\")\n",
    "y_conv = tf.matmul(x_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Probabilities - output from model (not the same as logits)\n",
    "y = tf.nn.softmax(y_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setup to test accuracy of model\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initilize all global variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200, training accuracy 0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400, training accuracy 0.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600, training accuracy 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800, training accuracy 0.97\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "# Run once to get the model to a good confidence level\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.964\n"
     ]
    }
   ],
   "source": [
    "# Run trained model against test data\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: mnist.test.images[0:500], \n",
    "                                                  y_: mnist.test.labels[0:500], keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_predictions(image_list):\n",
    "    prob = y.eval(feed_dict={x: image_list, keep_prob: 1.0})\n",
    "    \n",
    "    pct_list = np.zeros(len(image_list))\n",
    "    pred_list = np.argmax(prob, axis=1)\n",
    "    for i in range(len(prob)):\n",
    "        pct_list[i] = prob[i][pred_list[i]]\n",
    "        \n",
    "    return pred_list, pct_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_predictions(image_list, output_probs=False, adversarial=False):\n",
    "    '''\n",
    "    Evaluate images against trained model and plot images.\n",
    "    If adversarial == True, replace middle image title appropriately\n",
    "    Return probability list if output_probs == True\n",
    "    '''\n",
    "    prob = y.eval(feed_dict={x: image_list, keep_prob: 1.0})\n",
    "    \n",
    "    pred_list = np.zeros(len(image_list)).astype(int)\n",
    "    pct_list = np.zeros(len(image_list)).astype(int)\n",
    "    \n",
    "    # Setup image grid\n",
    "    import math\n",
    "    cols = 3\n",
    "    rows = math.ceil(image_list.shape[0]/cols)\n",
    "    fig = plt.figure(1, (12., 12.))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(rows, cols),  # creates grid of axes\n",
    "                     axes_pad=0.5,  # pad between axes in inch.\n",
    "                     )\n",
    "    \n",
    "    # Get probs, images and populate grid\n",
    "    for i in range(len(prob)):\n",
    "        pred_list[i] = np.argmax(prob[i]) # for mnist index == classification\n",
    "        pct_list[i] = prob[i][pred_list[i]] * 100\n",
    "\n",
    "        image = image_list[i].reshape(28,28)\n",
    "        grid[i].imshow(image)\n",
    "        \n",
    "        grid[i].set_title('Label: {0} \\nCertainty: {1}%' \\\n",
    "                          .format(pred_list[i], \n",
    "                                  pct_list[i]))\n",
    "        \n",
    "        # Only use when plotting original, partial deriv and adversarial images\n",
    "        if (adversarial) & (i % 3 == 1): \n",
    "            grid[i].set_title(\"Adversarial \\nPartial Derivatives\")\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    return prob if output_probs else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get 10 2s [:,2] from top 500 [0:500], nonzero returns tuple, get index[0], then first 10 [0:10]\n",
    "index_of_2s = np.nonzero(mnist.test.labels[0:500][:,2])[0][0:10]\n",
    "x_batch = mnist.test.images[index_of_2s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(x_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All above 2s are correctly classified, although at varying degrees of certainty (mostly above 95%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand on benchmark model using low-level API. Create adversarial image of a 2 to be misclassified as a 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Mostly inspired by:\n",
    "# https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture\n",
    "def create_plot_adversarial_images(x_image, y_label, lr=0.1, n_steps=1, output_probs=False):\n",
    "    \n",
    "    original_image = x_image\n",
    "    probs_per_step = []\n",
    "    \n",
    "    # Calculate loss, derivative and create adversarial image\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/train/gradient_computation\n",
    "    loss =  tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_conv)\n",
    "    deriv = tf.gradients(loss, x)\n",
    "    image_adv = tf.stop_gradient(x - tf.sign(deriv)*lr/n_steps)\n",
    "    image_adv = tf.clip_by_value(image_adv, 0, 1) # prevents -ve values creating 'real' image\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Calculate derivative and adversarial image\n",
    "        dydx = sess.run(deriv, {x: x_image, keep_prob: 1.0}) # can't seem to access 'deriv' w/o running this\n",
    "        x_adv = sess.run(image_adv, {x: x_image, keep_prob: 1.0})\n",
    "        \n",
    "        # Create darray of 3 images - orig, noise/delta, adversarial\n",
    "        x_image = np.reshape(x_adv, (1, 784))\n",
    "        img_adv_list = original_image\n",
    "        img_adv_list = np.append(img_adv_list, dydx[0], axis=0)\n",
    "        img_adv_list = np.append(img_adv_list, x_image, axis=0)\n",
    "\n",
    "        # Print/plot images and return probabilities\n",
    "        probs = plot_predictions(img_adv_list, output_probs=output_probs, adversarial=True)\n",
    "        probs_per_step.append(probs) if output_probs else None\n",
    "    \n",
    "    return probs_per_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_adversarial_image(x_image, y_label, lr=0.1, n_steps=1):\n",
    "    \n",
    "    original_image = x_image\n",
    "    probs_per_step = []\n",
    "    \n",
    "    # Calculate loss, derivative and create adversarial image\n",
    "    # https://www.tensorflow.org/versions/r0.11/api_docs/python/train/gradient_computation\n",
    "    loss =  tf.nn.softmax_cross_entropy_with_logits(labels=y_label, logits=y_conv)\n",
    "    deriv = tf.gradients(loss, x)\n",
    "    image_adv = tf.stop_gradient(x - tf.sign(deriv)*lr/n_steps)\n",
    "    image_adv = tf.clip_by_value(image_adv, 0, 1) # prevents -ve values creating 'real' image\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # Calculate derivative and adversarial image\n",
    "        dydx = sess.run(deriv, {x: x_image, keep_prob: 1.0}) # can't seem to access 'deriv' w/o running this\n",
    "        x_adv = sess.run(image_adv, {x: x_image, keep_prob: 1.0})\n",
    "        x_image = np.reshape(x_adv, (1, 784))\n",
    "        \n",
    "    label, prob = create_predictions(x_image)\n",
    "    return x_image, label, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pick a random 2 image from first 1000 images \n",
    "# Create adversarial image and with target label 6\n",
    "index_of_2s = np.nonzero(mnist.test.labels[0:1000][:,2])[0]\n",
    "rand_index = np.random.randint(0, len(index_of_2s))\n",
    "image_norm = mnist.test.images[index_of_2s[rand_index]]\n",
    "image_norm = np.reshape(image_norm, (1, 784))\n",
    "label_adv = [0,0,0,0,0,0,1,0,0,0] # one hot encoded, adversarial label 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_specific_adversarial_image(originNumber, destinationNumber, n_steps=10):\n",
    "    index_of_number = np.nonzero(mnist.test.labels[0:1000][:,originNumber])[0]\n",
    "    rand_index = np.random.randint(0, len(index_of_number))\n",
    "    image_norm = mnist.test.images[index_of_number[rand_index]]\n",
    "    image_norm = np.reshape(image_norm, (1, 784))    \n",
    "    label_adv = np.zeros(10).astype(int) # one hot encoded\n",
    "    label_adv[destinationNumber] = 1;\n",
    "    adv_image, label, prob = create_adversarial_image(image_norm, label_adv, lr=0.2, n_steps=n_steps)\n",
    "    return adv_image, label, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_trainSet():\n",
    "    number_of_examples_per_number = 1;\n",
    "    original_number = np.zeros(shape=(1, 1))\n",
    "    original_number = np.delete(original_number, 0, 0)\n",
    "    target_number = np.zeros(shape=(1, 10))\n",
    "    target_number = np.delete(target_number, 0, 0)\n",
    "    prob_list = np.zeros(shape=(1, 1))\n",
    "    prob_list = np.delete(prob_list, 0, 0)\n",
    "    adv_img_list = np.zeros(shape=(1, 784))\n",
    "    adv_img_list = np.delete(adv_img_list, 0, 0)\n",
    "    for origin in range(3):\n",
    "        for p in range(number_of_examples_per_number):\n",
    "            for target in range(3):\n",
    "                if origin != target:\n",
    "                    prob = 0\n",
    "                    label = origin\n",
    "                    n_steps = 10\n",
    "                    while prob < 0.9 or label != target:\n",
    "                        adv_image, label, prob = create_specific_adversarial_image(origin, target, n_steps)\n",
    "                        prob = prob[0]\n",
    "                        label = label[0]\n",
    "                        print(\"-\", end=\"\", flush=True)\n",
    "                        \n",
    "                        #print((origin, target, label, prob, n_steps), end=\"\", flush=True)                        \n",
    "                        #print(\" X \")\n",
    "                        n_steps = n_steps + 2\n",
    "                    original_number = np.append(original_number, origin)\n",
    "                    target_to_add = np.zeros(shape=(1,10))\n",
    "                    target_to_add[0][target] = 1 #to vectorize the labeling\n",
    "                    target_number = np.concatenate((target_number, target_to_add), axis=0)\n",
    "                    prob_list = np.append(prob_list, prob)\n",
    "                    #the action of concation can be i,proved\n",
    "                    adv_img_list = np.concatenate((adv_img_list, adv_image), axis=0)\n",
    "                    print(\"|\", end=\"\", flush=True)\n",
    "        print(\"Number\")            \n",
    "        #print(original_number, target_number, prob_list)\n",
    "    return original_number, target_number, prob_list, adv_img_list\n",
    "\n",
    "\n",
    "def generate_adversarial_and_save():\n",
    "    original_number, target_number, prob_list, adv_img_list = generate_trainSet()\n",
    "    np.save('original_number.npy', original_number)\n",
    "    np.save('target_number.npy', target_number)\n",
    "    np.save('prob_list.npy', prob_list)\n",
    "    np.save('adv_img_list.npy', adv_img_list)\n",
    "    \n",
    "    \n",
    "def load_saved_adversarial():    \n",
    "    original_number = np.load('original_number.npy')\n",
    "    target_number = np.load('target_number.npy')\n",
    "    prob_list = np.load('prob_list.npy')\n",
    "    adv_img_list = np.load('adv_img_list.npy')\n",
    "    return original_number, target_number, prob_list, adv_img_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number\n"
     ]
    }
   ],
   "source": [
    "original_number, target_number, prob_list, adv_img_list = generate_trainSet()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniting and shuffeling between adv inputs and benign\n",
    "# creating labels as 2d array which [0,1] is adv\n",
    "number_of_adv_images = adv_img_list.shape[0]\n",
    "adv_label = np.zeros(number_of_adv_images)\n",
    "adv_label = np.c_[adv_label, np.ones(number_of_adv_images)]\n",
    "\n",
    "number_of_benign = 1000\n",
    "benign_label = np.ones(number_of_benign)\n",
    "benign_label = np.c_[benign_label, np.zeros(number_of_benign)]\n",
    "# uniting adv and benign\n",
    "united_labels = np.concatenate((adv_label, benign_label), axis=0)\n",
    "united_images = np.concatenate((adv_img_list, mnist.train.images[0:number_of_benign]), axis=0)\n",
    "united_images_and_labels = np.c_[united_images, united_labels]\n",
    "np.random.shuffle(united_images_and_labels)\n",
    "#add precondition\n",
    "images_data, labels_data = np.hsplit(united_images_and_labels, [united_images_and_labels.shape[1]-2])\n",
    "train_size = 700\n",
    "image_train_data, image_test_data = np.split(images_data, [train_size])\n",
    "label_train_data, label_test_data = np.split(labels_data, [train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 32, 64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\alonh\\Documents\\Thesis\\MNIST-adversarial-images\\original-nn-data.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "# reset and restore old variables\n",
    "local_path = \"C:\\\\Users\\\\alonh\\Documents\\\\Thesis\\\\MNIST-adversarial-images\\\\original-nn-data.ckpt\"\n",
    "# Initilize all global variables\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "print(W_conv2.shape)\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, local_path)\n",
    "saver = None\n",
    "tf.reset_default_graph()\n",
    "W_conv1 = tf.get_variable(\"W_conv1\", shape=W_conv1.shape)\n",
    "b_conv1 = tf.get_variable(\"b_conv1\", shape=b_conv1.shape)\n",
    "W_conv2 = tf.get_variable(\"W_conv2\", shape=W_conv2.shape)\n",
    "b_conv2 = tf.get_variable(\"b_conv2\", shape=b_conv2.shape)\n",
    "saver = tf.train.Saver(var_list={'W_conv1': W_conv1, 'b_conv1': b_conv1, 'W_conv2': W_conv2, 'b_conv2': b_conv2})\n",
    "saver.restore(sess, \"C:\\\\Users\\\\alonh\\Documents\\\\Thesis\\\\MNIST-adversarial-images\\\\original-nn-data.ckpt\")\n",
    "print(W_conv2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders nodes for images and label inputs\n",
    "# Create placeholders nodes for images and label inputs\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784], name='xDetector')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2], name='yDetector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "# modified nn based on original\n",
    "to_initialize = []\n",
    "# Input layer\n",
    "x_image_new = tf.reshape(x, [-1,28,28,1]) # mnist image comes in as 784 vector\n",
    "\n",
    "# # Conv layer 1 - 32x5x5 - using same weights and biases as the pretrained\n",
    "# W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# b_conv1 = bias_variable([32])\n",
    "x_conv1 = tf.nn.relu(conv2d(x_image_new, W_conv1) + b_conv1)\n",
    "x_pool1 = max_pooling_2x2(x_conv1)\n",
    "# Conv detector layer 1  - 32x5x5\n",
    "W_conv1_detector = weight_variable([5, 5, 32, 32])\n",
    "b_conv1_detector = bias_variable([32])\n",
    "x_conv1_detector = tf.nn.relu(conv2d(x_pool1, W_conv1_detector) + b_conv1_detector)\n",
    "x_pool1_detector = max_pooling_2x2(x_conv1_detector)\n",
    "\n",
    "# # Conv layer 2 - 64x5x5\n",
    "# W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "# b_conv2 = bias_variable([64])\n",
    "print(W_conv2.shape)\n",
    "x_conv2 = tf.nn.relu(conv2d(x_pool1, W_conv2) + b_conv2)\n",
    "x_pool2 = max_pooling_2x2(x_conv2)\n",
    "\n",
    "# stacking conv2 output and x_pool1 detector output\n",
    "conv_detector_2_input = tf.concat([x_conv2, x_pool1], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conv detector layer 2 - 64x5x5\n",
    "W_conv2_detector = weight_variable([5, 5, 96, 64])\n",
    "b_conv2_detector = bias_variable([64])\n",
    "x_conv2_detector = tf.nn.relu(conv2d(conv_detector_2_input, W_conv2_detector) + b_conv2_detector)\n",
    "x_pool2_detector = max_pooling_2x2(x_conv2_detector)\n",
    "# Flatten - keras 'flatten'\n",
    "pool_shape = x_pool2_detector.shape\n",
    "shape_size = (pool_shape[1]*pool_shape[2]*pool_shape[3]).value\n",
    "x_flat_detector = tf.reshape(x_pool2_detector, [-1, shape_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "W_fc1_detector = weight_variable([shape_size, 1024]) # max pooling reduced image to 7x7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_fc1_detector = bias_variable([1024])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_fc1_detector = tf.nn.relu(tf.matmul(x_flat_detector, W_fc1_detector) + b_fc1_detector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"dropout_1/random_uniform:0\", shape=(?, 1024), dtype=float32) must be from the same graph as Tensor(\"Placeholder:0\", dtype=float32).",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-430-1fc40cb0900f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Regularization with dropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkeep_prob_detector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx_fc1_drop_detector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_fc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob_detector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(x, keep_prob, noise_shape, seed, name)\u001b[0m\n\u001b[0;32m   2335\u001b[0m     \u001b[0mrandom_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2336\u001b[0m     random_tensor += random_ops.random_uniform(\n\u001b[1;32m-> 2337\u001b[1;33m         noise_shape, seed=seed, dtype=x.dtype)\n\u001b[0m\u001b[0;32m   2338\u001b[0m     \u001b[1;31m# 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2339\u001b[0m     \u001b[0mbinary_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbinary_op_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5769\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5770\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5771\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   5426\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5427\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5428\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5429\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5430\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   5362\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5363\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[1;32m-> 5364\u001b[1;33m                                                                 original_item))\n\u001b[0m\u001b[0;32m   5365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"dropout_1/random_uniform:0\", shape=(?, 1024), dtype=float32) must be from the same graph as Tensor(\"Placeholder:0\", dtype=float32)."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# Regularization with dropout\n",
    "keep_prob_detector = tf.placeholder(tf.float32)\n",
    "x_fc1_drop_detector = tf.nn.dropout(x_fc1, keep_prob_detector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Variable_6:0\", shape=(1024, 2), dtype=float32_ref) must be from the same graph as Tensor(\"dropout_1/mul:0\", shape=(?, 1024), dtype=float32).",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-431-19b0b8456b93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mW_fc2_detector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mb_fc2_detector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbias_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my_conv\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_fc1_drop_detector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_fc2_detector\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_fc2_detector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   1948\u001b[0m       \u001b[0mare\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mset\u001b[0m \u001b[0mto\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1949\u001b[0m   \"\"\"\n\u001b[1;32m-> 1950\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1951\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1952\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Only one of transpose_a and adjoint_a can be True.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5769\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5770\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5771\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   5426\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5427\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5428\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5429\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5430\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   5362\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5363\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[1;32m-> 5364\u001b[1;33m                                                                 original_item))\n\u001b[0m\u001b[0;32m   5365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"Variable_6:0\", shape=(1024, 2), dtype=float32_ref) must be from the same graph as Tensor(\"dropout_1/mul:0\", shape=(?, 1024), dtype=float32)."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "# Classification layer\n",
    "W_fc2_detector = weight_variable([1024, 2])\n",
    "b_fc2_detector = bias_variable([2])\n",
    "y_conv= tf.matmul(x_fc1_drop_detector, W_fc2_detector) + b_fc2_detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities - output from model (not the same as logits)\n",
    "# CHANGE!!-----------------------------------------------------------------\n",
    "y = tf.nn.softmax(y_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"yDetector:0\", shape=(?, 2), dtype=float32) must be from the same graph as Tensor(\"add_25:0\", shape=(?, 10), dtype=float32).",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-433-fe956c1ac428>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loss and optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcross_entropy2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_conv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# add variables to optimize here:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mW_conv1_detector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_conv2_detector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW_fc2_detector\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_step2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_entropy2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m               instructions)\n\u001b[1;32m--> 272\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    274\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[1;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[0;32m   1962\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m   with ops.name_scope(name, \"softmax_cross_entropy_with_logits_sg\",\n\u001b[1;32m-> 1964\u001b[1;33m                       [logits, labels]) as name:\n\u001b[0m\u001b[0;32m   1965\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"labels_stop_gradient\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5769\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5770\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5771\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5772\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   5426\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5427\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5428\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5429\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5430\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   5362\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5363\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[1;32m-> 5364\u001b[1;33m                                                                 original_item))\n\u001b[0m\u001b[0;32m   5365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"yDetector:0\", shape=(?, 2), dtype=float32) must be from the same graph as Tensor(\"add_25:0\", shape=(?, 10), dtype=float32)."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "cross_entropy2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "# add variables to optimize here:\n",
    "var_list = [W_conv1_detector, W_conv2_detector, W_fc2_detector]\n",
    "train_step2 = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy2, var_list=var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"ArgMax:0\", shape=(?,), dtype=int64) must be from the same graph as Tensor(\"ArgMax_3:0\", shape=(?,), dtype=int64).",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-434-378d25bbdf13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Setup to test accuracy of model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorrect_prediction2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_conv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0maccuracy2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect_prediction2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mequal\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   2860\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2861\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 2862\u001b[1;33m         \"Equal\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m   2863\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2864\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[1;31m# Need to flatten all the arguments into a list.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_Flatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   5426\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5427\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5428\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5429\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5430\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   5362\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5363\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[1;32m-> 5364\u001b[1;33m                                                                 original_item))\n\u001b[0m\u001b[0;32m   5365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"ArgMax:0\", shape=(?,), dtype=int64) must be from the same graph as Tensor(\"ArgMax_3:0\", shape=(?,), dtype=int64)."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Setup to test accuracy of model\n",
    "correct_prediction2 = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor. (Operation name: \"init\"\nop: \"NoOp\"\n is not an element of this graph.)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 282\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    283\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3338\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3339\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3422\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3423\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Operation %s is not an element of this graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3424\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Operation name: \"init\"\nop: \"NoOp\"\n is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-435-830677080d03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Initilize all uninitialized variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minit_new_vars_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_initialize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_new_vars_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# checking didnt missed ant variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1085\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    287\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[1;32m--> 289\u001b[1;33m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[0;32m    290\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[1;31mValueError\u001b[0m: Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor. (Operation name: \"init\"\nop: \"NoOp\"\n is not an element of this graph.)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Initilize all uninitialized variables\n",
    "init_new_vars_op = tf.initialize_variables(to_initialize)\n",
    "sess.run(init_new_vars_op)\n",
    "\n",
    "# checking didnt missed ant variable\n",
    "uninitialized_vars = []\n",
    "for var in tf.all_variables():\n",
    "    try:\n",
    "        sess.run(var)\n",
    "    except tf.errors.FailedPreconditionError:\n",
    "        uninitialized_vars.append(var)\n",
    "\n",
    "if uninitialized_vars.count()!=0:\n",
    "    raise Exception('uninitialized variables: ', uninitialized_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-436-c61e0821ed00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mbatch_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_train_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m20\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"step %d, training accuracy %g\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     \"\"\"\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alonh\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4940\u001b[0m                        \"`eval(session=sess)`\")\n\u001b[0;32m   4941\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4942\u001b[1;33m       raise ValueError(\"Cannot use the default session to evaluate tensor: \"\n\u001b[0m\u001b[0;32m   4943\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4944\u001b[0m                        \u001b[1;34m\"graph. Pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Train model\n",
    "# Run once to get the model to a good confidence level\n",
    "batch_size = 100\n",
    "batch_index = 0\n",
    "for i in range(100):\n",
    "    if(batch_size>=image_train_data.shape[0]):\n",
    "        print(\"reached limit of train data current index: \", batch_index, \"data size: \", train_data.shape[0])\n",
    "        break\n",
    "    batch_image = image_train_data[batch_index: (batch_index + batch_size)]\n",
    "    batch_label = label_train_data[batch_index: (batch_index + batch_size)]\n",
    "    if i%20 == 0:\n",
    "        train_accuracy = accuracy2.eval(feed_dict={x: batch_image, y_: batch_label, keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "    train_step2.run(feed_dict={x: batch_image, y_: batch_label, keep_prob: 0.4})\n",
    "    batch_index = batch_index + batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "batch_image = image_train_data[0: 100]\n",
    "print(batch_image[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-438-68a4b72878c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run trained model against test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: test_data[0], \n\u001b[0m\u001b[0;32m      3\u001b[0m                                                   y_: test_data[1], keep_prob: 1.0}))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Run trained model against test data\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x: test_data[0], \n",
    "                                                  y_: test_data[1], keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
